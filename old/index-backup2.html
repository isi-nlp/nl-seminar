<html>
<head>
  <style type="text/css">
  <!--
    A:link    { text-decoration: none; color: #000099}
    A:active  { text-decoration: none; color: #000099}
    A:visited { text-decoration: none; color: #000099} 
    A:hover   { text-decoration: none; color: #990099}
  //-->
  </style>

<script type="text/javascript">
<!--
function exp_coll(ind) {
 s = document.getElementById(ind);

 if (s.style.display == 'none') {
   s.style.display = 'block';
 } else if (s.style.display == 'block') {
   s.style.display = 'none';
 }
}
-->
</script>

<title>

NL Seminar

</title>
</head>
<body bgcolor="#F7F7FF" text="#000033" link="#000099" vlink="#000099" alink="#000099">


<center><font size="5" face="Verdana, Arial, Helvetica, sans-serif">
NL Seminar
</font></center>

<p>

The NL Seminar is a weekly meeting of the <a
href="http://www.isi.edu/natural-language/">Natural
Language Group</a> here at <a href="http://www.isi.edu/">ISI</a>.  The
seminars usually take place on Fridays from 3:00pm until 4:00pm,
though exceptions are made for visitors, etc.  Contact <a
href="mailto:sdene(remove-spam}efe@isi.edu">Steve DeNeefe</a> to schedule a
talk.  Non Natural Language Group members may receive seminar announcements by subscribing to the nlg-seminar list <a href="http://mailman.isi.edu/mailman/listinfo/nlg-seminar">here</a>.

<p>An iCal feed is now available at <a href="http://www.isi.edu/natural-language/nl-seminar/nl.ics"><tt>http://www.isi.edu/natural-language/nl-seminar/nl.ics</tt></a><p><p>

Click on the titles to view the abstracts.

</p><p>


<p>
Note: Outside visitors should go to the tenth floor lobby where
they will be met and escorted to the appropriate location
immediately before the talk.
<p>
<h3>Upcoming talks:</h3>

<table width=90% border=2 cellspacing=1 cellpadding=1 align=center>
  <tr bgcolor=#DDDDFF><td align=left width=14%>
    <b>Date</b>
  </td><td align=left width=25%>
    <b>Speaker</b>
  </td><td align=left>
    <b>Title</b>
  </td></tr>

  <tr><td align=left valign=top>
    15 Aug 08
  </td><td align=left valign=top>
    Kyle Gorman (Penn)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Aug_15');">
    Intern Final Talk:  The Entropy of English given French
    </a><br>
  <span id=abs08_Aug_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The fundamental task in statistical machine translation (SMT) is to  
characterize the probability of a target sentence given its source  
translation; for translating French as English, P(f | e). By applying  
Bayes Rule, we derive the fundamental theorem of SMT: e maximizing  
P(e) P(f | e). Advances in SMT come from improving estimations of  
these two terms, or from more efficient ways of searching for optimal  
solutions (Brown et al. 1993).<p>
In the case of language modeling, Shannon (1949) and Brown et al.  
(1992) identified upper and lower bounds for the per-character entropy  
of English, H(e), for humans and machines, respectively. We ask the  
same question for SMT, H(e | f), comparing the results for human  
translators and a simple machine baseline based on IBM Model 1. These  
numbers are the upper and lower bounds for SMT systems trained on  
parallel data.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Aug 08
  </td><td align=left valign=top>
    John DeNero (Berkeley)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Aug_15b');">
    Intern Final Talk: Minimum Risk Decoding over Forests
    </a><br>
  <span id=abs08_Aug_15b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:45 pm - 4:15 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Minimum Bayes risk (MBR) decoding improves the output of
machine translation systems by selecting a translation that matches a
large proportion of the k-best hypotheses of a system.  We extend this
idea to apply to packed forests by selecting an output sentence that
matches a large proportion of all hypotheses in the pruned forest of
derivations from a syntax-based translation system.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Aug 08
  </td><td align=left valign=top>
    Catalin Tirnauca (Univ. Rovira i Virgili)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Aug_22');">
    Intern Final Talk:  On the Consistency of Probabilistic Context-Free Grammars
    </a><br>
  <span id=abs08_Aug_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Probabilistic context-free grammars can describe probability distributions over strings, i.e., the sum of probabilities of all generated strings is 1.This condition is often  called consistency. It has applications in fields of natural language processing such as probabilistic parsing (disambiguate by picking the parse with the highest score), or speech recognition (rank hypotheses returned by a speech recognizer). 
 
The talk is a survey of some of the previous results. We investigate how we can determine if a probabilistic context-free grammar is consistent, and if such a test can always be done. Also, we study a method, namely normalization, which guarantees consistent probabilistic context-free grammars. Moreover, we mention briefly some techniques that train probabilistic context-free grammars and guarantee consistency.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Aug 08
  </td><td align=left valign=top>
    Amittai Axelrod (UW)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Aug_22b');">
    Intern Final Talk
    </a><br>
  <span id=abs08_Aug_22b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:45 pm - 4:15 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
abstract coming soon...

</font>
</span>
</td></tr>
</table>


<h3>Past talks:</h3>

<table width=90% border=2 cellspacing=1 cellpadding=1 align=center>
  <tr bgcolor=#DDDDFF><td align=left width=14%>
    <b>Date</b>
  </td><td align=left width=25%>
    <b>Speaker</b>
  </td><td align=left>
    <b>Title</b>
  </td></tr>

  <tr><td align=left valign=top>
    18 Jul 08
  </td><td align=left valign=top>
    Sujith Ravi
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jul_18');">
    Deciphering Ciphers Optimally Using Only Minimal Knowledge of the Source Language
    </a><br>
  <span id=abs08_Jul_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will be talking about deciphering letter-substitution ciphers *optimally* using only minimal knowledge (bigrams, trigrams, etc.) of the source language, instead of relying on large look-up dictionaries. We also plan to show how our empirical results compare with Shannon's predictions on the equivocation curves and unicity distance measure.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Jul 08
  </td><td align=left valign=top>
    Jon May
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jul_11');">
    Thesis Proposal Practice Talk:  A Weighted Tree Transducer Toolkit for Syntactic Natural Language Processing Models
    </a><br>
  <span id=abs08_Jul_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Solutions for many natural language processing problems such as speech recognition, transliteration, and  translation have been described as weighted finite-state transducer cascades. The transducer formalism is very useful for researchers, not only for its ability to expose the deep similarities between seemingly disparate models, but also because expressing models in this formalism allows for rapid implementation of real, data-driven systems. Finite-state toolkits can interpret and process transducer chains using generic algorithms and many real-world systems have been built using these toolkits. Current research in NLP makes use of syntax-rich models that are poorly suited to extant transducer toolkits, which process linear input and output. Tree transducers can handle these models, and a weighted tree transducer toolkit with appropriate generic algorithms will lead to the sort of gains in syntax-based modeling that were achieved with string transducer toolkits. In this thesis proposal practice talk I will briefly trace the history of finite-state transducers and automata as they relate to natural language processing and the evolution of formalisms and the toolkits that support them, leading up to motivation for the design and creation of Tiburon, the toolkit referenced in this talk's title. I will describe previous, current, and future work on Tiburon's algorithms and the effectiveness of both algorithms and  software at cleanly representing syntax-based NLP models from the literature and at constructing and evaluating novel models.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    13 Jun 08
  </td><td align=left valign=top>
    Ellen Riloff
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jun_13');">
    Effective Information Extraction with Relevant Regions and Semantic Affinity Patterns
    </a><br>
  <span id=abs08_Jun_13 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will briefly overview the landscape of event-oriented information
extraction (IE) systems and explain why it is especially challenging
to learn IE systems without annotated training data. Then I will
describe one attempt to do so by decoupling the tasks of finding
relevant text regions and applying extraction patterns. First, a
self-trained relevant sentence classifier identifies relevant regions
in documents. Second, a "semantic affinity" measure identifies
domain-relevant extraction patterns.  We further distinguish between
"primary" patterns and "secondary" patterns and apply the patterns
selectively in the relevant regions.  This approach is weakly
supervised, requiring only a few seed patterns plus relevant and
irrelevant (but unannotated) documents for training.  The resulting IE
system achieves reasonably good performance, despite the fact that the
relevant region classifier leaves a lot to be desired.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    06 Jun 08
  </td><td align=left valign=top>
    Tom Murray (USC)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jun_06');">
    Knowledge as a Constraint on Uncertainty for Unsupervised Classification
    </a><br>
  <span id=abs08_Jun_06 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk investigates the use of domain knowledge to constrain and improve the unsupervised learning of a classifier, by placing limits or biases on the possible hypotheses for each input. Theoretically, we view the contribution of the knowledge source as a reduction in the uncertainty of the model's decisions, quantified by the resulting conditional entropy of the label distribution given the input corpus. Evaluating on the simple case of an unsupervised HMM tagger, we find surprising levels of improvement from little knowledge, with more stable and efficient training convergence and label assignment, and a high degree of correlation between classification entropy and model performance. We conclude that, while we should always seek better generic models and techniques, for applications in an unsupervised setting, knowledge may still be key.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 May 08
  </td><td align=left valign=top>
    Steve DeNeefe
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_May_30');">
    BLEU Sway Issues: one way to get statistical significance, two ways to get a better score, and three ways to thwart them
    </a><br>
  <span id=abs08_May_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
BLEU the de facto standard for evaluation and development of statistical machine translation systems.  We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in BLEU scores that are questionable or even absurd. We propose a very conservative modification to BLEU that addresses these issues while improving correlation with human judgements, then explore some deeper modifications that alleviate the problems further.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 May 08
  </td><td align=left valign=top>
    David Newman (UCI)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_May_16');">
    Theory and Applications of Topic Modeling
    </a><br>
  <span id=abs08_May_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Topic models, a class of Bayesian probabilistic models for discrete
data, have recently gained popularity in applications ranging from
document modeling to computer vision.  Since the introduction of
Latent Dirichlet Allocation (LDA) in 2003, there have been numerous
extensions to this archetype.  I will review the theory behind LDA,
and discuss subsequent models, including (some of): Correlated Topic
Model, Dynamic Topic Model, Hierarchical Topic Model, Special Words
Topic Model, Hierarchical Dirichlet Process Model, Pachinko Allocation
Machine, Topics and Syntax Model, Bi-LDA, Author-Topic Model,
Supervised Topic Model, Spatial LDA, etc. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 May 08
  </td><td align=left valign=top>
    John DeNero (Berkeley)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_May_09');">
    Inference in phrase alignment models
    </a><br>
  <span id=abs08_May_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Models that align phrases instead of words offer an
appealing alternative to the standard relative frequency estimates of
phrase translation probabilities.  But, while some effective word
alignment models (Model 1, Model 2 & HMM) can be estimated tractably
with EM, phrase alignment models cannot.  I'll talk about how to show
that estimation and inference under these models is intractable.
Then, I'll present two useful approximation techniques.<p>
First, I'll talk about how to cast phrase alignment search as an
integer linear programming (ILP) problem and find the optimal
alignment reliably and quickly with off-the-shelf ILP software.  Some
applications of this technique include training phrase alignment
models and interpreting the output of word alignment models.<p>
Second, we'll look at how to estimate translation probabilities under
a phrase alignment model using a Gibbs sampling procedure.  The
sampler has some nice asymptotic convergence properties and also seems
to produce good results in practice. I'll walk through the different
models we've trained and how they performed.<p>
Time permitting, I'll also talk about some of the ways in which we
could potentially extend this work to syntactic MT.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 May 08
  </td><td align=left valign=top>
    Zornitsa Kozareva
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_May_02');">
    Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs
    </a><br>
  <span id=abs08_May_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We present a novel approach to weakly supervised semantic class learning from
the web, using a single powerful hyponym pattern combined with graph
structures, which capture two properties associated with pattern-based
extractions: popularity and productivity. Intuitively, a candidate is popular
if it was discovered many times by other instances in the hyponym pattern. A
candidate is productive if it frequently leads to the discovery of other
instances. Together, these two measures capture not only frequency of
occurrence, but also cross-checking that the candidate occurs both near the
class name and near other class members. We developed two algorithms that begin
with just a class name and one seed instance and then automatically generate a
ranked list of new class instances. We conducted experiments on four semantic
classes and consistently achieved high accuracies.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Apr 08
  </td><td align=left valign=top>
    David Chiang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Apr_25');">
    Tutorial: Randomized data structures for large statistical NLP models
    </a><br>
  <span id=abs08_Apr_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Randomized algorithms are those which use randomness to achieve efficient performance with a bounded probability of error; typically, the bound is adjustable and the performance depends on the bound. Randomized data structures, likewise, use randomness to achieve efficient storage with a bounded probability of error. I will give an overview of the use of such data structures, namely, Bloom filters and "Bloomier" filters, for storing very large n-gram language models, and will discuss possibilities for using randomized data structures for other purposes as well. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Apr 08
  </td><td align=left valign=top>
    Rahul Bhagat
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Apr_18');">
    Learning Paraphrases from Text
    </a><br>
  <span id=abs08_Apr_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Paraphrases are textual expressions that convey the same meaning using different words. They capture variability, which is a common phenomenon in language. Given this, paraphrases have been shown to be useful in many natural language applications like Question-Answering, Machine Translation, Summarization and Information Retrieval. In this talk, I'll discuss the phenomenon paraphrasing and focus on methods for automatically acquiring paraphrases from text.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Apr 08
  </td><td align=left valign=top>
    Jon May
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Apr_11');">
    Syntactic Re-Alignment Models for Machine Translation
    </a><br>
  <span id=abs08_Apr_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We present a method for improving word alignment for statistical syntax-based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly-used word alignment models. This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    04 Apr 08
  </td><td align=left valign=top>
    Ulf Hermjakob
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Apr_04');">
    Name Translation in Statistical Machine Translation: Learning When to Transliterate
    </a><br>
  <span id=abs08_Apr_04 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We present a method to transliterate names in the framework of
end-to-end statistical machine translation. The system is trained to
learn when to transliterate.<p>
For Arabic to English MT, we developed and trained a transliterator on a
bitext of 7 million sentences and Google's English terabyte ngrams and
achieved better name translation accuracy than 3 out of 4 professional
translators. The talk also includes a discussion of challenges in name
translation evaluation.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Mar 08
  </td><td align=left valign=top>
    Jason Riesa
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Mar_25');">
    Tutorial on Arabic Orthography
    </a><br>
  <span id=abs08_Mar_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 11:30 am<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This tutorial is intended to provide attendees with working knowledge of the Arabic writing system. No previous experience with Arabic is required. At the end of this tutorial you should be able to read and segment individual Arabic characters, read common ligatures, identify possible affixes on stems, and understand the various lexical normalizations used in Arabic text preprocessing. The focus will be on the formal writing system in printed text for Modern Standard Arabic, although handwriting will be briefly discussed.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Jan 08
  </td><td align=left valign=top>
    Victoria Fossum
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jan_18');">
    Using Syntax to Improve Word Alignment Precision for Syntactic Machine Translation
    </a><br>
  <span id=abs08_Jan_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Automatically word-aligning a parallel bitext in the source and target languages constitutes the first stage of most statistical machine translation pipelines.  Automatic word alignment is error-prone, and produces many incorrect links.  Incorrect links that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntactic machine translation.  We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules.  We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments, relative to a GIZA++ union baseline.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Jan 08
  </td><td align=left valign=top>
    Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs08_Jan_11');">
    How to Make EM Do What You Want
    </a><br>
  <span id=abs08_Jan_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I'll talk about some unsupervised learning experiments -- how I was satisfied with the initial results, how I became very dissatisfied, and how I became (somewhat) satisified again. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    14 Dec 07
  </td><td align=left valign=top>
    Marieke van Erp
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Dec_14');">
    MITCH: Mining for Information in Texts from the Cultural Heritage
    </a><br>
  <span id=abs07_Dec_14 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Naturalis, the Dutch National Museum of Natural History, harbours one of the largest treasures of the world: the key specimens of millions of animals found throughout the world through centuries of biological expeditions. While the depot where the animals are stored is a technical marvel, Noah's ark of the 21st century, it is hard to search through it. Research in taxonomy, the evolution of life and biodiversity revolves around the specimens in the depot. The main key to accessing the depot are(mostly) handwritten expedition logs and registration books, which are currently being photographed and keyed in to be stored in searchable digital archives. Such digital logs already enable a kind of "Biogoogle" search, but actual research questions are more complicated ("how did this kind of frog develop over the last century in the Amazon rainforests?"), and demand more intelligent handling. This is where the MITCH project comes in.<p>
The goal of MITCH is to turn the field logs and registration books into a populated semantic network, in which concepts such as animal specimens are related to all other concepts that define them: where, when, under which circumstances and by whom were they found, who described them first in the academic literature, who prepared them for storage in the Naturalis depot, which registration number was assigned to them, etc. This means that all textual descriptions of a specimen need to be parsed into exactly these concepts and their relations. All of this needs to be  done at a scale that goes far beyond the human capacity, as tens of thousands of digitized but unanalysed textual records are waiting for semantic analysis. This necessitates the use of state-of-the-art machine learning methods that learn from examples automatically.<p>
The project addresses its goals on three levels. The basic level is the development and application of automatic data cleaning and markup tools. On top of this, semi-structured textual material such as fieldbook logs and scientific papers, are semi-automatically converted to a searchable knowledge base. Search results are visualised by displaying maps and specimen photos. The conversion phase assumes the active intervention of domain experts, such as collection managers, to correct and steer the automatic extraction  procedure. At the top level, information resources are cross-linked using a domain ontology, populating a semantic network that can be hooked up to any other standardised cultural heritage knowledge base or to a search engine. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 Nov 07
  </td><td align=left valign=top>
    Bill Rounds (Michigan and Stanford)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Nov_02');">
    Constructions, Constraints, Transducers, and TAGs: A unifying view through Feature Logic
    </a><br>
  <span id=abs07_Nov_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The value of mathematical formalisms for speech recognition, language generation, and machine translation has long been recognized. Not so much work, though, has been spent reconciling these formalisms with linguistic theories. In this talk I'll propose a theoretical descriptive mechanism based on feature logic, which is central to construction and constraint-based linguistic theories like construction grammar and HPSG, and which  can be used to view tree transducers and tree-adjoining grammars as giving rise to a construction-based framework.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    19 Oct 07
  </td><td align=left valign=top>
    Slav Petrov (Berkeley)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Oct_19');">
    Learning and Inference for Hierarchically Split PCFGs
    </a><br>
  <span id=abs07_Oct_19 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 11:30 am<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Treebank parsing can be seen as the search for an optimally refined
grammar consistent with a coarse training treebank. We describe a
method in which a minimal grammar is hierarchically refined using EM
to give accurate, compact grammars. The resulting grammars are
extremely compact compared to other high-performance parsers, yet the
parser gives the best published accuracies on several languages, as
well as the best generative parsing numbers in English. In addition,
we give an associated coarse-to-fine inference scheme which vastly
improves inference time with no loss in test set accuracy. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Oct 07
  </td><td align=left valign=top>
    Jon Patrick (Univ. of Sydney)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Oct_17');">
    Enhancement Technologies for ICU Information Systems
    </a><br>
  <span id=abs07_Oct_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The School of Information Technologies at the University of Sydney has
had a 3 year partnership with the Intensive Care Unit at the Royal
Prince Alfred Hospital, Sydney. In that time they have managed 8 joint
projects aimed at producing software solutions that enhance
productivity in the Unit and in some cases enabled entirely new
functionalities in their information systems. The principle motivation
for the research is the processing of the narratives in clinical notes
but concomitant problems in information systems have also been tackled
and the combination of the two disciplines have led to the two related
processing systems to be described in this presentation.<p>

- Ward Rounds Information Systems (WRIS) & Handovers -
The WRIS is designed to support the work of all clinical staff in
their ward rounds activities. The system, when activated,
automatically populates from the resident clinical database a pro
forma report with the most recent relevant data about the patient,
such as vital signs, pathology reports, and other diagnostic
measurements, presented as a web page. The clinical staff then write
their progress notes into the web page which converts the text to
SNOMED CT codes and other relevant concepts and entities. The
clinician is given the opportunity to change any analyses done by the
processor. This clinician approved data is loaded to the patient
record. The essential elements of this system, that is computing an
extract of the patient record, accepting narrative input, and
analysing the text for coding, is a productivity gain of itself, but
more importantly, also constitutes the beginning of a hospital wide
Handovers System for use throughout each step in the patient
journey. This system is being tested at the RPAH ICU in readiness for
ward usage. The impact of this system in improving the quality and
safety of handovers has the potential to be very significant.<p>

- Clinical Data Analytics Language (CDAL) -
General purpose access to data from clinical information systems,
beyond retrieval for point of care work, is needed for many aspects of
the hospital's work particularly for clinical research, logistics &
operational planning, and auditing patient safety. Most current
clinical systems only provide access to data identified in standard
reports with no flexibility to make ad hoc enquiries or to pursue new
directions of enquiry. The clinical data analytics language developed
enables the expression of any question that can be answered from the
data in the database in a restricted natural language. A prototype of
the language has been developed for the CareVue information system
used in the ICU at the Royal Prince Alfred Hospital. It provides for
the use of local medical dialects, SNOMED CT terminology including all
forms of collective expressions in SNOMED (e.g. infectious diseases),
specification of patient groups, a variety of statistical functions,
and constraints over any medical variable, Time, and Location. CDAL is
general in that it can be bolted on to any clinical information system
and is applicable to any clinical specialisation.<p>
<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Oct 07
  </td><td align=left valign=top>
    David Talbot (Edinburgh)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Oct_12');">
    Scalable Language Modeling: Breaking the Curse of Dimensionality
    </a><br>
  <span id=abs07_Oct_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Randomized data structures can help us scale discrete models encountered in NLP. This talk will describe their use in language modeling and present some more general related results.<p>
N-gram language models are fundamental to speech recognition and machine translation. Unfortunately, the n-gram parameter space grows exponentially with the dimension of the feature vector. I will describe how randomization can be used to remove the space-dependency of such models on the a priori parameter space.<p>
The novel extensions of the Bloom filter that I will present are able to take advantage of the entropy of the distribution of values assigned to feature vectors to save space in a discrete statistical model. I will review some results applying these models to language modeling in machine translation and relate their space-requirements to a novel lower bound on the general problem of querying a map of key/value pairs.<p>
No prior knowledge of randomized data structures will be assumed.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Oct 07
  </td><td align=left valign=top>
    Sujith Ravi
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Oct_05');">
    Will this parser work with my data? - Predicting Parser Accuracy without Gold-Standard information
    </a><br>
  <span id=abs07_Oct_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
There are many tools available to the NLP community for Natural Language Parsing, (i.e converting a raw sentence in to a parse-tree). NLP researchers usually use some "off-the-shelf" parser which has been trained on the Wall Street Journal (WSJ) corpora and then apply the WSJ-trained parser to their data. This works in many cases, especially for systems which use data from WSJ or similar corpora. However, in real life applications, the data may be compiled from many different sources and span different genres, and may not be similar to the WSJ corpora in terms of sentence structure, etc . A particular parser might parse well on some corpora and not so well on others. Choosing the right parser for your data may have an impact on the performance of the NLP system as a whole. But in order to measure the accuracy of any parser for a given corpus, we require a set of gold-standard parse trees corresponding to the sentences within the corpus. Generating gold-standard set takes a lot of manual work and in many real-life applications, it is not a feasible  task to generate gold-standard parses for large corpora.<p>
We attempted to build a system which can predict the accuracy (in terms of f-measure value) of the Charniak parser (a popular parsing tool) on any given sentence corpus. Without using any additional information (i.e gold std. parses), our system predicts "how accurately the Charniak parser could parse the given corpus". In order to evaluate our system's predictions on a particular corpus, we compute the "Correlation" measure between the "actual accuracies (using Gold-standard)" vs. "predicted accuracies (from our system)" for the given corpus. We tested our system on different corpora and using different methods and will present these results.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    29 Aug 07
  </td><td align=left valign=top>
    Carmen Heger (Dresden) and Michael Bloodgood (Delaware)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Aug_29');">
    Summer Intern Presentations: Composition of Tree Transducers AND Using the Perceptron Algorithm to Tune Large Numbers of Feature Weights for Syntax-Based Statistical Machine Translation
    </a><br>
  <span id=abs07_Aug_29 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Composition of Tree Transducers<p>
Since finite state (string) transducers are not expressive enough for many NLP 
applications, computational linguistics started to investigate tree 
transducers for the task of machine translation, for example. Quite some 
successful work has been done on generalizing results from string transducers 
to tree transducers. But when it comes to composition results are not 
satisfying because generally tree transducers are not closed under 
composition. Still we think that most of the tree transducers used in NLP are 
composable and that is why we defined the problem of the composition for two 
individual transducers instead of the whole class. During the summer we 
started with linear nondeleting tree transducers with epsilon rules and 
approached an algorithm to decide for two such transducers whether their 
composition is again in the same class.<p>
Using the Perceptron Algorithm to Tune Large Numbers of Feature Weights for Syntax-Based Statistical Machine Translation<p>
Current state-of-the-art syntax-based statistical machine translation
systems produce many candidate translations out of which the output translation
is selected by taking the argmax over all candidates i of &lt;w,f_i&gt; where w is a 
weight vector and f_i is a vector of the feature values for candidate i. The
features used by the system and their corresponding weights have a major impact
on a system's performance.  Currently, Minimum Error Rate Training (MERT) is used to
tune the weights of the features.  A drawback of this is that it isn't tractable
to tune large numbers of feature weights.  I will discuss using the perceptron 
algorithm to tune feature weights for statistical machine translation.  If I get interesting
results before my talk, I may also dicsuss new classes of features (potentially very large 
numbers of features) that can be used for improving MT performance.  

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Aug 07
  </td><td align=left valign=top>
    Wei Ho (Princeton) and Jennifer Gillenwater (Rice)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Aug_24');">
    Summer Intern Presentations: Noisy Language Models AND Context for Syntax-Based Translation Rules
    </a><br>
  <span id=abs07_Aug_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Noisy Language Models<p>
The language models used in statistical machine translation are often
quite large, requiring significant memory and sometimes pre-processing
in order to be utilized effectively. It would be desirable to have a
more compact representations of language models while minimizing the
impact on translation quality. Various quantization methods and lossy
storage of language models will be presented.<p>
Context for Syntax-Based Translation Rules<p>
The rules that a translation system employs should be applicable in
many contexts.  This ensures that a rich language is expressible with
a minimum number of rules.  However, when rules that are applicable in
too many contexts are combined, they result in nonsensical
translations.  How can we keep rules general but constrain the context
of their use?  This summer we explored the approach of constraining
the context by conditioning on various neighboring elements of each
rule. <p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Aug 07
  </td><td align=left valign=top>
    Anoop Sarkar (Simon Fraser)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Aug_16');">
    Extensions of Regular Tree Grammars and their relation to Tree Adjoining Grammars
    </a><br>
  <span id=abs07_Aug_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
There is a hierarchy of generative devices that generate trees:
starting with regular tree languages (RTLs), which are contained
within context-free tree languages (CFTLs), and so on.  The string
yield of the RTLs is exactly the set of Context-Free Languages,
while the yield of the CFTLs is exactly the set of Indexed Languages.
In this talk we introduce Adjoining Tree Languages (ATLs) which sit
in between RTLs and CFTLs.<p>
The yield of ATGs is exactly the set of Tree-Adjoining Languages.
Just like RTGs are stronger than CFGs, ATGs are stronger than TAGs.
In addition we will show that the ATG notation simplifies many of
the foundational proofs for TAGs including proofs of the closure
properties. In particular, ATLs do not use adjunction constraints,
and thus are much easier to understand than TAGs.<p>
We compare ATGs with previously proposed simplifications of CFTGs,
called monadic simple CFTGs, which also have been shown to be weakly
equivalent to TAG (i.e. they generate the same set of string
languages). We consider the question of whether these two weakly
equivalent formalisms are strongly equivalent (i.e. generate exactly
the same set of tree languages).<p>
Finally, we will show that the standard definition used for
probabilistic TAG is (surprisingly) very different from the natural
definition of probabilistic ATL. Using an example of PP-attachment
ambiguity we show that the two probabilistic models are different
from each other. <p>
About the speaker:<p>
Anoop Sarkar is an assistant professor in the Department of Computing
Science at Simon Fraser University. He received his PhD in 2002
from the Department of Computer and Information Science at the
University of Pennsylvania, with Prof. Aravind Joshi as his advisor.
His research work is on machine learning, especially semi-supervised
learning, applied to the processing of natural language and stochastic
formal grammars.<p>
Anoop Sarkar's web-page: <a href=http://www.cs.sfu.ca/~anoop/> http://www.cs.sfu.ca/~anoop</a>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Jun 07
  </td><td align=left valign=top>
    Donghui Feng
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_15b');">
    Extracting Data Records from Unstructured Biomedical Full Text
    </a><br>
  <span id=abs07_Jun_15b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 11:00 am - 11:30 am<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this paper, we address the problem of extracting data records and
their attributes from unstructured biomedical full text. There has
been little effort reported on this in the research community. We
argue that semantics is important for record extraction or
finer-grained language processing tasks. We derive a data record
template including semantic language models from unstruc-tured text
and represent them with a dis-course level Conditional Random Fields
(CRF) model. We evaluate the approach from the perspective of
Information Extrac-tion and achieve significant improvements on system
performance compared with other baseline systems. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Jun 07
  </td><td align=left valign=top>
    Alex Fraser
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_15');">
    Getting the structure right for word alignment: LEAF
    </a><br>
  <span id=abs07_Jun_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 11:00 am<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Automatic word alignment is the problem of automatically annotating
parallel text with translational correspondence. Previous generative
word alignment models have made structural assumptions such as the
1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while
previous discriminative models have either made one of these
assumptions directly or used features derived from a generative model
using one of these assumptions. We present a new generative alignment
model which avoids these structural limitations, and show that it is
effective when trained using both unsupervised and semi-supervised
training methods. Experiments show strong improvements in word
alignment accuracy and usage of the generated alignments in
hierarchical and phrasal SMT systems improves the BLEU score.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    08 Jun 07
  </td><td align=left valign=top>
    Liang-Chih Yu (Cheng Kung U)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_08');">
    Topic Analysis for Psychiatric Document Retrieval (Practice Talk for ACL)
    </a><br>
  <span id=abs07_Jun_08 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Psychiatric document retrieval attempts to help people to efficiently
and effectively locate the consultation documents relevant to their
depressive problems. Individuals can understand how to alleviate their
symptoms according to recommendations in the relevant documents. This
work proposes the use of high-level topic information extracted from
consultation documents to improve the precision of retrieval
results. The topic information adopted herein includes negative life
events, depressive symptoms and semantic relations between symptoms,
which are beneficial for better understanding of users'
queries. Experimental results show that the proposed approach achieves
higher precision than the word-based retrieval models, namely the
vector space model (VSM) and Okapi model, adopting word-level
information alone.  <p>
About the speaker:<p>
Liang-Chih Yu (<a href=http://www.isi.edu/~liangchi>http://www.isi.edu/~liangchi</a>) is
now a visiting student in the Information Sciences Institute (ISI) of
University of Southern California (USC). My host advisor is Dr. Eduard
Hovy. I am also a PhD candidate in the Department of Computer Science
and Information Engineering, National Cheng Kung University, Tainan,
Taiwan. My advisor is Dr. Chung-Hsien Wu. My research interests
include natural language processing, text mining, information
retrieval, ontology construction, spoken dialogue system. <p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    08 Jun 07
  </td><td align=left valign=top>
    Jonathan May
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_08b');">
    Bisimulation Minimisation for Weighted Tree Automata
    </a><br>
  <span id=abs07_Jun_08b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We describe existing forward and backward bisimulation minimisation
algorithms for nondeterministic automata and extend these algorithms
to weighted tree automata. The extended algorithms, which work for all
semirings, retain the time complexity of their counterparts for
unweighted tree automata for additively cancellative semirings, and
are only slightly higher (linear instead of logarithmic in the number
of states) on other semirings. We describe the effectiveness of an
implementation of these algorithms on a typical task in natural
language processing. <p>
This is joint work with Johanna Hgberg, Ume University and Andreas
Maletti, Technische Universitt Dresden.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    01 Jun 07
  </td><td align=left valign=top>
    Jingbo Zhu
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_01');">
    Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem
    </a><br>
  <span id=abs07_Jun_01 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this paper, we analyze the effect of resampling techniques,
including under-sampling and over-sampling used in active learning for
word sense disambiguation (WSD). Experimental results show that
under-sampling causes negative effects on active learning, but
over-sampling is a relatively good choice. To alleviate the
within-class imbalance problem of over-sampling, we propose a
bootstrap-based over-sampling (BootOS) method that works better than
ordinary over-sampling in active learning for WSD. Finally, we
investigate when to stop active learning, and adopt two strategies,
max-confidence and min-error, as stopping conditions for active
learning. According to experimental results, we sug-gest a prediction
solution by considering max-confidence as the upper bound and
min-error as the lower bound for stopping conditions. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    01 Jun 07
  </td><td align=left valign=top>
    Andrew S. Gordon
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jun_01b');">
    Generalizing Semantic Role Annotations Across Syntactically Similar Verbs
    </a><br>
  <span id=abs07_Jun_01b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Large corpora of parsed sentences with semantic role labels (e.g. PropBank)
provide training data for use in the creation of high-performance automatic
semantic role labeling systems. Despite the size of these corpora,
individual verbs (or rolesets) often have only a handful of instances in
these corpora, and only a fraction of English verbs have even a single
annotation. In this paper, we describe an approach for dealing with this
sparse data problem, enabling accurate semantic role labeling for novel
verbs (rolesets) with only a single training example. Our approach involves
the identification of syntactically similar verbs found in PropBank, the
alignment of arguments in their corresponding rolesets, and the use of their
corresponding annotations in PropBank as surrogate training data.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 May 07
  </td><td align=left valign=top>
    Wei Wang (Language Weaver)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_May_25');">
    Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy
    </a><br>
  <span id=abs07_May_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We show that phrase structures in Penn Treebank style parses
are not optimal for syntax-based machine translation. We
exploit a series of binarization methods to restructure the
Peen Treebank style trees such that syntactified phrases
smaller than Penn Treebank constituents can be acquired and
exploited in translation. We find that by employing the EM
algorithm for determining the binarization of a parse tree
among a set of alternative binarizations gives us the best
translation result.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 May 07
  </td><td align=left valign=top>
    Feng Pan
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_May_18');">
    Computing Semantic Similarity between Skill Statements for Approximate Matching
    </a><br>
  <span id=abs07_May_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(This will be an extended version of the talk for NAACL-HLT 2007. It's
based on my summer internship work at IBM T.J. Watson Research Center
last year.) <p>
The project aimed to address the problems encountered when trying to
match available employees to open job positions, based on skill
matches. Currently, job search applications, like IBM's Professional
Marketplace, only find exact matches. A skill affinity computation is
desired to allow searches to be expanded to related/similar skills,
and return more potential matches. <p>
In this talk, I will explore the problem of computing text similarity
between verb phrases describing skilled human behavior for the purpose
of finding approximate matches. Four parsers (Charniak's parser,
Stanford's parser, IBM XSG slot grammar parser, and Lin's MINIPAR) are
evaluated on a corpus of skill statements extracted from an
enterprise-wide expertise taxonomy. A similarity measure utilizing
common semantic role features extracted from parse trees was found
superior to an information-theoretic measure of similarity and
comparable to the level of human agreement. <p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 May 07
  </td><td align=left valign=top>
    Steve DeNeefe
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_May_11');">
    What Can Syntax-based MT Learn from Phrase-based MT?
    </a><br>
  <span id=abs07_May_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We compare and contrast the strengths and weaknesses of a syntax-based
machine translation model with a phrase-based machine translation
model on several levels.  We briefly describe each model, highlighting
points where they differ.  We include a quantitative comparison of the
phrase pairs that each model has to work with, as well as the reasons
why some phrase pairs are not learned by the syntax-based model.  We
then propose improvements to the syntax-based extraction techniques to
capture more phrases.  We also compare the translation accuracy for
all variations. 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    04 May 07
  </td><td align=left valign=top>
    Sheelagh Carpendale (Calgary)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_May_04');">
    Information Visualization and Collaboration
    </a><br>
  <span id=abs07_May_04 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Consider Donald Norman's quote, "The power of the unaided mind is
highly overrated. Without external aids, memory, thought, and
reasoning are all constrained. But human intelligence is highly
flexible and adaptive, superb at inventing procedures and objects that
overcome its own limits. The real powers come from devising external
aids that enhance cognitive abilities." (Norman, 1993) Common methods
for externalization include making sketches on whatever happens to be
handy -- paper napkins, program margins, etc. -- and/or finding a
colleague or two to discuss the problem with. It would seem then, that
visualization and collaboration are natural possibilities for creating
positive cognitive aids. I will discuss our approach to developing
interactive information visualizations both to support individuals and
small groups of collaborators and briefly describe some of our recent
results. <p>
About the speaker:<p>
Sheelagh Carpendale holds a Canada Research Chair in Information
Visualization at the University of Calgary. Her research focuses on
the visualization, exploration and manipulation of information;
visualizing such topics as ecological dynamics, uncertainty in
information, social and communication information and investigating
the development of information visualization environments that support
collaboration. Dr. Carpendale's research in information visualization
and interaction design draws on her dual background in Computer
Science (BSc. and Ph.D. Simon Fraser University) and Visual Arts
(Sheridan College, School of Design and Emily Carr, College of Art). 

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    20 Apr 07
  </td><td align=left valign=top>
    Christopher Collins (Toronto)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Apr_20');">
    Information Visualization to Support Computational Linguistics
    </a><br>
  <span id=abs07_Apr_20 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We present a survey of resent research into using information visualization
to reveal new insights about linguistic data.  Our recent work includes
using WordNet hyponymy as a basis for document visualization and visualizing
the uncertainty in machine translation in an instant messaging chat
context.  We will present our preliminary findings and prototype
visualization for machine translation data resulting from a week of
collaboration with ISI researchers.<p>
About the speaker:<p>
Christopher Collins is a PhD candidate in information visualization and
computational linguistics at the University of Toronto.  He works with Prof.
Gerald Penn and Prof. Sheelagh Carpendale (University of Calgary).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 Mar 07
  </td><td align=left valign=top>
    Ido Dagan (Bar-Ilan U)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Mar_30');">
    Textual entailment as a framework for applied semantics
    </a><br>
  <span id=abs07_Mar_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We have recently proposed Recognizing Textual Entailment (RTE) as a
generic task that captures major semantic inferences across different
natural language processing applications. The talk will first review
the motivation and definition of the textual entailment task and the
PASCAL RTE-1,2&3 Challenges benchmarks. Then we will demonstrate
directions for building textual entailment systems, based on knowledge
acquisition and inference, and for utilizing them within concrete
applications. Furthermore, we suggest that textual entailment modeling
may become a comprehensive framework for applied semantics
research. Such framework introduces useful variants of known semantic
problems and highlights important tasks which were hardly investigated
so far at an applied computational level. The semantic modeling
perspective will be illustrated in more detail by a case study for an
entailment-based variant of word sense disambiguation. <p>
About the speaker:<p>
Ido Dagan is a Senior Lecturer at the Department of Computer Science
at Bar Ilan University, Israel. His areas of interest are largely
within empirical NLP, particularly empirical approaches for applied
semantic processing. In the last few years Ido and his colleagues
introduced <i>textual entailment</i> as a generic framework for applied
semantic inference and have organized the first three rounds of the
PASCAL Recognizing Textual Entailment Challenges. Ido received his
Ph.D. from the Technion. He has been a research fellow at the IBM
Haifa Scientific Center and a Member of Technical Staff at AT&T Bell
Laboratories. During 1998-2003 he was co-founder and CTO of
FocusEngine and VP of Technology of LingoMotors.  

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Mar 07
  </td><td align=left valign=top>
    Hermann Helbig (U at Hagen, Germany)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Mar_23');">
    Multilayered Extended Semantic Networks as a Knowledge Representation Paradigm and Interlingua for Meaning Representation 
    </a><br>
  <span id=abs07_Mar_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4 CR<br>
<b>Abstract:</b> <br>
The talk gives an overview of  Multilayered Extended Semantic Networks
(abbreviated MultiNet), which is one of the most comprehensively
described knowledge representation paradigms used as a semantic
interlingua in large-scale NLP applications and for linguistic
investigations into the semantics and pragmatics of natural
language. As with other semantic networks, concepts are represented in
MultiNet by nodes, and relations between concepts are represented as
arcs between these nodes. Additionally to that, every node is
classified according to a predefined conceptual ontology forming a
hierarchy of sorts, and the nodes are embedded in a multidimensional
space of layer attributes and their values. MultiNet provides a set of
about 150 standardized relations and functions which are described in
a very concise way including an axiomatic apparatus, where the axioms
are classified according to predefined types. The representational
means of MultiNet claim to fulfill the criteria of universality,
homogeneity, and cognitive adequacy. In the talk, it is also shown,
how MultiNet can be used for the semantic representation of different
semantic phenomena. To overcome the quantitative barrier in building
large knowledge bases and semantically oriented computational lexica,
MultiNet is associated with a set of tools including a semantic
interpreter NatLink for automatically translating natural language
expressions into MultiNet networks, a workbench LIA for the computer
lexicographer, and a workbench MWR for the knowledge engineer for
managing and graphically manipulating semantic networks. The
applications of MultiNet as a semantic interlingua range from natural
language interfaces to the Internet and to dedicated databases, over
question-answering systems, to systems for automatic knowledge
acquisition.<p>
About the speaker:<p>
Prof. Helbig is head of the chair Intelligent Information and Communication 
Systems at the University of Hagen, Germany. His main research areas are
Knowledge Representation, Semantic Natural Language Processing, and 
Question-Answering.<p>
A CV can be found <a href="slides/CV-En-HH.pdf"> here</a>.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 Mar 07
  </td><td align=left valign=top>
    Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Mar_09');">
    The Voynich Manuscript
    </a><br>
  <span id=abs07_Mar_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The medieval Voynich Manuscript has been called "the most 
mysterious document in the world".  Its pages contain bizarre drawings 
of strange plants and astrological diagrams, as well as an undeciphered 
script of 20,000 running words, written in a character set that has never 
been seen elsewhere.  Its origin is also controversial, with many theories 
abounding.  I will describe the document, show samples, explain where it 
may have come from, and present some properties of the text. <p>
This will more of a history/mystery talk than 
a computer science talk.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    26 Jan 07
  </td><td align=left valign=top>
    Gerald Penn (Toronto)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jan_26');">
    The Quantitative Study of Writing Systems
    </a><br>
  <span id=abs07_Jan_26 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
If you understood all of the world's languages, you would still not be
able to read many of the texts that you find on the world wide web,
because they are written in non-Roman scripts -- often ones that have
been arbitrarily encoded for electronic transmission in the absence of
an accepted standard.  This very modern nuisance reflects a dilemma as
ancient as writing itself: the association between a language as it is
spoken and its written form has a sort of internal logic to it that we
can comprehend, but the conventions are different in every individual
case --- even among languages that use the same script, or between
scripts used by the same language.  This conventional association
between language and script, called a <i>writing system</i>, is indeed
reminiscent of the Saussurean conception of language itself, a
conventional association of meaning and sound, upon which modern
linguistic theory is based.  Despite linguists' reliance upon writing
to present and preserve linguistic data, however, writing systems were
a largely forgotten corner of linguistics until the 1960s, when Gelb
presented their first classification.<p>
This talk will describe recent work that aims to place the study of
writing systems upon a sound computational and statistical foundation.
While archaeological decipherment may eternally remain the holy grail
of this area of research, it also has applications to speech
synthesis, machine translation, and multilingual document retrieval.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Jan 07
  </td><td align=left valign=top>
    Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jan_12');">
    Capturing Natural Language Transformations
    </a><br>
  <span id=abs07_Jan_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Knowledge representation is hard.  As natural language scientists and
engineers, we'd like something that <p>
- is expressive enough to capture how natural language works<p>
- permits tractable inference<p>
- admits learning algorithms for automatic knowledge acquisition<p>
- leads to modular system construction<p>
This talk will look at knowledge representation for capturing natural
language transformations.  A lot of what we do falls into this
category.  Examples of transformations include language translation
(French to English), question answering (Question to Answer),
transliteration (foreign script to Roman alphabet), summarization
(long text to short text), parsing (string to tree), language
generation (meaning to string), etc. <p>
I'll show various knowledge formats (starting with simple finite-state
transducers) and show how they stack up on the 4 criteria above, using
theorems and examples.  We'll see that different types of tree and
string automata lead to good behavior on various subsets of the 4
criteria, but getting 4 out of 4 is still elusive. <p>
This is a Krazy Theory talk -- since this kind of talk should not go
on and on, I promise to finish within 50 minutes.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Jan 07
  </td><td align=left valign=top>
    Beata Klebanov (Hebrew U)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs07_Jan_05');">
    Experimental and Computational Investigation of Lexical Cohesion in Texts
    </a><br>
  <span id=abs07_Jan_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Lexical cohesion refers to structure created in a text by use of words with
related meanings. Apart from its importance in theoretical and applied
linguistics, lexical cohesion detection is used in NLP tasks like topic
segmentation, extractive summarization, spelling correction, etc.  However, the
intuitive potential of lexical cohesion for such tasks is often not realized in
practice, possibly due to shortcomings of detection algorithms.<p>
I will briefly describe an experiment with readers aimed at providing reliable
data for a computational investigation of lexical cohesion. We then discuss a
number of informative features for cohesion detection, drawing on sources like
WordNet, distributional information, free associations, and the structure of
information in  the text itself.  Finally, I report experiments 
with supervised learning of lexical cohesion. <p>
About the speaker:<p>
Beata Beigman Klebanov is a PhD candidate at the Hebrew University of Jerusalem,
Israel, currently a visiting scholar at Northwestern University. Beata's 
interests are in experimental, computational and applied research in text
pragmatics.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Dec 06
  </td><td align=left valign=top>
    Jerry Hobbs
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Dec_15');">
    When Will Computers Understand Shakespeare?
    </a><br>
  <span id=abs06_Dec_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this talk I will examine problems encountered in coming to some
kind of understanding of one sonnet by Shakespeare (his 64th), ask
what it would take to solve these problems computationally, and
suggests routes to the solution.  The general conclusion is that we
are closer to this goal as one might think.  Or are we?<p>
Bio:<p>
Jerry Hobbs is famous primarily for having an office next to Kevin
Knight's and a parking space next to Ed Hovy's.  He has read
everything of Shakespeare's that survives, including his will and
plays of dubious authorship.  But that was all a long time ago.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    14 Dec 06
  </td><td align=left valign=top>
    Liang Huang (Penn)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Dec_14');">
    Faster Decoding with Synchronous Grammars and n-gram Language Models
    </a><br>
  <span id=abs06_Dec_14 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 1:30 pm - 3:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
A major obstacle in syntax-based machine translation is the  
prohibitively large search space for decoding with an integrated  
language model. We develop faster approaches for this problem based  
on lazy algorithms for k-best parsing. When comparing against  
Chiang's technique of cube pruning, our method runs up to twice as  
fast without making more search errors or decreasing translation  
accuracy as measured by BLEU. We demonstrate the effectiveness of the  
algorithm on a large-scale translation system.<p>
Interestingly, these techniques can be applied to speed up bilexical  
parsing as well, where the (bi-) lexical probabilities can be viewed  
as n-gram probabilities that causes non-monotonicity. This method  
fits naturally into the coarse-to-fine grained multi-pass parsing  
schemes.<p>
To push this direction even further, we can generalize cube and lazy  
cube pruning as generic tools for reducing complicated search spaces,  
as alternatives to the well-known A* and annealing techniques.<p>
This is joint work with David Chiang (ISI).

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 Nov 06
  </td><td align=left valign=top>
    Mark Hopkins (Potsdam)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Nov_27');">
    Towards the Effective Exploitation of Syntax in Machine Translation
    </a><br>
  <span id=abs06_Nov_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We discuss preliminary work on a possible approach to exploiting
syntax in an effective way for machine translation. The driving
guideline is to devise a machine translation system that can perform
effectively, given a very limited quantity of parsed training data.  

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Nov 06
  </td><td align=left valign=top>
    David DeVault (Rutgers)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Nov_17');">
    Scorekeeping in an Uncertain Language Game
    </a><br>
  <span id=abs06_Nov_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Practical dialogue systems must exploit context to interpret user
utterances correctly.  Received views of context and coordination in
pragmatic theory equate utterance context with the occurrent
subjective states of interlocutors using notions like common knowledge
or mutual belief.  We argue that these views are not well suited for
practical modeling due to the uncertainty and robustness of context
dependence in human-human dialogue.  We present an alternative
characterization of utterance context as objective and normative.  On
this view, an interlocutor's representation of context reflects
private uncertainty about the true objective context as determined by
prior speaker meanings.  As conversation moves forward, new utterances
provide interlocutors with retrospective insight about each other's
prior meanings and therefore about what the true context really is.
This view reconciles the need for uncertainty with received intuitions
about coordination, and can directly inform computational approaches
to dialogue.<p>
Joint work with Matthew Stone, Rutgers and Rich Thomason, Michigan<p>
About the Speaker:<p>
David DeVault is a Ph.D. candidate in the Department of Computer
Science at Rutgers University.  He holds a B.S. in Engineering and
Applied Science from the California Institute of Technology and an
M.A. in Philosophy from Rutgers University.  David's research aims to
develop techniques to allow computational agents to participate in
flexible task-oriented conversations with human beings.  His recent
work has drawn on design challenges encountered in building such an
agent to try to articulate practical, learnable, and theoretically
satisfying representations of context, utterance meaning, and speaker
intention for implemented conversational systems.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    03 Nov 06
  </td><td align=left valign=top>
    Jens-Soenke Voeckler
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Nov_03');">
    perl part 2 - advanced magick
    </a><br>
  <span id=abs06_Nov_03 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Since part 1 of the Perl tutorial didn't cover the juicy bits (like a  
unique function in Perl), based on feedback from participants, I am  
offering a part 2 "Perl - Advanced Magick" covering:<p>
o the slides from roughly page 40
    - The Schwartzian Transform
    - Dissecting a program
o What to do, if you do need popen or backticks?
o OO Perl - a start
o C embedding - definitely only a "start here"
o Useful recipes, e.g. interpolating variables in configuration  
scripts from Perl values.<p>
If there is something you are especially interested in seeing, please  
send me an email

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Oct 06
  </td><td align=left valign=top>
    Jens-Soenke Voeckler
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Oct_23');">
    perl - how to use it, not abuse it
    </a><br>
  <span id=abs06_Oct_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 12:00 pm - 1:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
If you speak a little perl, are an occasional perl-scripter, and  
would like to know more about how to use it as a (p)ortable, (e)
fficient, and (r)eadible (l)anguage, you may be interested in my  
brown bag (read: bring your own) lunch seminar:<p>
I will talk about using Perl in a portable fashion, the environment  
it is run in, and how avoid common mistakes and misconceptions. Perl  
offers more than a thousand ways to solve a problem, but some are  
more portable or more efficient than others. If time permits, simple  
hands-on examples can be tried out during the talk, so power for  
laptops will be provided.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    29 Sep 06
  </td><td align=left valign=top>
    Ashish Venugopal (CMU)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Sep_29');">
    Delayed LM Intersection and Left-to-Right N-Best Extraction for Syntax-Based MT
    </a><br>
  <span id=abs06_Sep_29 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We begin by describing a set of pruning constraints that are applied
in the literature to effectively restrict the search space of
synchronous PCFGs intersected with target language model contexts. We
apply these constraints to non-binarized grammars with a large number
of non-terminals and demonstrate effective parsing within the
framework of Wu, 97.  <p>
We then present a novel parsing approach that avoids language model
context intersection during parsing in favor of language model driven
n-best list extraction. The parsing step produces a sentence
spanning parse forest which is explored in left-to-right target order
by the N-Best extraction method. <p>
This method avoids lossy pruning during the parsing process, searching
a much larger effective parse space than practically possible in the
full intersection scenario, and has the important benefit of allowing
integration of a high order language within the N-Best search process,
rather than only in parse re-scoring.  <p>
We demonstrate the impact of this parsing approach using the SPCFG
approach described in Zollmann, Venugopal, Vogel 06, which is similar
to Galley et al., 04 and compare performance against full
intersection.  <p>
This is joint work with Andreas Zollmann<p>
About the Speaker:<p>
Ashish Venugopal is a Ph.D candidate at the Language Technologies
Institute at Carnegie Mellon University, and holds B.S (SCS,
Univ. Honors), M.S degrees from the same institution. He is a Seibel
Scholar and has received the annual Graduate Student Teaching Award at
Carnegie Mellon. His research focus is on syntax augmented machine
translation. <p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Sep 06
  </td><td align=left valign=top>
    Eduard Hovy
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Sep_22');">
    Toward a 'Science' of Annotation: Experiences from OntoNotes 
    </a><br>
  <span id=abs06_Sep_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
As machine learning algorithms and their application for NLP become
better understood, attention turns toward the production of annotated
corpora to which they can be applied.  Numerous phenomena present
themselves for annotation, including aspects in lexical semantics,
discourse, pragmatics, and dialogue.  But several questions
immediately must be answered:  <p>
1. How does one obtain a balanced corpus to annotate?  What is a
balanced corpus?  <p>
2. How does one decide which aspects to annotate? How does one
adequately express the theory behind the phenomena in simple annotation steps? <p>
3. Which annotators does one hire?  How does one ensure that they are adequately trained?  <p>
4. How does one establish a simple, fast, and trustworthy annotation
procedure?  What interfaces does one build?  How does one ensure that
the interfaces do not affect the annotation results?  <p>
5. How does evaluate the results? What are the appropriate agreement
measures?  At which cutoff points should one re-do the annotations?
How does one ensure improvement?<p>
6. How should one formulate and store the results?  How does one
ensure compatibility with other existing resources?  How does one make
results available for best impact?  <p>
7. How does one report the annotation effort and results?  How does
one actually get a paper on this work published at an important
conference?  What should the paper contain?   <p>
Despite their being so basic, there is almost no established procedure
or standard set of answers to these questions today.  In this talk I
discuss some of these aspects, pointing to the lessons learned in the
ongoing OntoNotes project (joint with BBN, the University of Colorado
(PropBank), the University of Pennsylvania (Treebank), and ISI).

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Aug 06
  </td><td align=left valign=top>
    Victoria Fossum (Michigan)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_25');">
    Improving Precision of Word Alignments Using GHKM Syntax-Based Rule Extraction
    </a><br>
  <span id=abs06_Aug_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Noisy word alignments negatively affect the quality of the translation 
rules extracted by the ISI syntax-based MT system.  In the literature, 
alignment is typically treated as a separate process from subsequent 
stages in the MT pipeline.  By contrast, we allow rule extraction to 
guide the alignment process.<p>
We present an unsupervised algorithm for identifying and removing "bad" 
links using GHKM syntax-based rule extraction.  We show that
we can improve upon the precision of GIZA union (measured against a gold 
standard set of manually aligned Chinese-English sentence pairs),
while only decreasing recall slightly.<p>

(Note: This is part of the Summer Intern Series)

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Aug 06
  </td><td align=left valign=top>
    Jason Riesa
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_25b');">
    Minimally Supervised Morphological Segmentation with Applications to Machine Translation
    </a><br>
  <span id=abs06_Aug_25b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Inflected languages in a low-resource setting present a data sparsity problem for
statistical machine translation. In this work, we present a minimally  
supervised algorithm for morpheme segmentation on Arabic dialects  
which reduces unknown words at translation time by over 50%, total  
vocabulary size by over 40%, and yields a significant increase in  
BLEU score over a previous state-of-the-art phrase-based statistical MT system.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Aug 06
  </td><td align=left valign=top>
    Joseph Turian (NYU)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_23b');">
    Speeding-up Syntax-based Decoding
    </a><br>
  <span id=abs06_Aug_23b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>
(Note: This is part of the Summer Intern Series)

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Aug 06
  </td><td align=left valign=top>
    Oana-Diana Postolache
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_23');">
    Towards combining Searn and Syntax-Based Machine Translation (SBMT)
    </a><br>
  <span id=abs06_Aug_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk is about modeling the Syntax-Based Machine Translation  
(SBMT) problem within the Searn (Search & Learn) framework developed by Hal Daume in  
his PhD thesis. I will present the way we define the states, actions
and the search space and how to implement the cost function.<p>

(Note: This is part of the Summer Intern Series)

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Aug 06
  </td><td align=left valign=top>
    Chenhai Xi
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_18');">
    Name Entity Transliteration Discovery from Large Bilingual Comparable Corpora
    </a><br>
  <span id=abs06_Aug_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this summer project, we investigate a scalable method to extract
Chinese-English name transliterations from large comparable corpora,  
which consist of two languages discussing same or similar topics. We show  
that bigram Jaccard coefficient is a good similarity method to compare English  
and Chinese names, at Chinese pronunciation (Pinyin) level. Based on this phonetic
similarity score, an efficient randomized algorithm is then used to  
find name pair candidates from English and Chinese lists. Finally, context  
information, such as dates, frequency, place and titles are combined with the  
phonetic similarity to improve the accuracy of the name pairs list.<p>
(Note: This is part of the Summer Intern Series)

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Aug 06
  </td><td align=left valign=top>
    Idan Szpektor (Bar-Ilan U)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_11');">
    Textual Entailment: Framework, Learning and Applications
    </a><br>
  <span id=abs06_Aug_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Textual Entailment has been proposed recently as a generic framework
for modeling semantic variability in many Natural Language Processing
applications, such as Question Answering, Information Extraction,
Information Retrieval and Document Summarization. The Textual
Entailment relationship holds between two text fragments, termed text
and hypothesis, if the truth of the hypothesis can be inferred from
the text.<p>
In this talk, the Textual Entailment framework will be introduced.
I'll then present an algorithm for large-scale Web-based acquisition
of entailment rules, a type of knowledge needed for robust inference.
Finally, I will present an unsupervised Relation Extraction approach
based on the Textual Entailment framework.<p>
About the speaker:<p>
Idan Szpektor is a PhD student under the supervision of Dr. Ido Dagan
at Bar Ilan University, Israel. His current research activity is in
acquisition of knowledge for textual entailment.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    04 Aug 06
  </td><td align=left valign=top>
    Shou-de Lin
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Aug_04');">
    Ph.D. defense practice talk
    </a><br>
  <span id=abs06_Aug_04 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This is a practice talk for my Ph.D. defense, which  
will be held on Aug 24th 3-5pm, SAL 322.<p>
An important problem in the area of homeland security and fraud  
detection is to identify abnormal entities in large datasets.   
Although there are methods from knowledge discovery and data mining  
focusing on finding anomalies in numerical datasets, there has been  
little work aimed at discovering abnormal or suspicious instances in  
large and complex semantic graphs whose nodes are richly connected  
with many different types of links. In this talk, I will describe a  
novel, domain-independent and unsupervised framework to identify such  
instances.  Besides discovering suspicious instances, we believe that  
to complete the discovery process and to deal with the "curse of  
false positives", a system has to convince the users by providing  
explanations for its findings. Therefore, in the second part of the  
talk I will describe an explanation mechanism to automatically  
generate human-understandable explanations for the discovered  
results. Experimental results show that our discovery system  
outperforms state-of-the-art unsupervised network algorithms used to  
analyze the 9/11 terrorist network by a large margin. Additionally, a  
human study we conducted demonstrates that our explanation system,  
which provides natural language explanations for its findings,  
allowed human subjects to perform complex data analysis in a much  
more efficient and accurate manner<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    28 Jul 06
  </td><td align=left valign=top>
    Qin Iris Wang (Alberta)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jul_28');">
    Improved Large Margin Dependency Parsing via Local Constraints and Laplacian Regularization
    </a><br>
  <span id=abs06_Jul_28 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk is about an improved approach for learning dependency parsers
from treebank data. Our technique is based on two ideas for improving
large margin training in the context of dependency parsing.  First, we
incorporate local constraints that enforce the correctness of each
individual link, rather than just scoring the global parse tree. Second,
to cope with sparse data, we smooth the lexical parameters according to
their underlying word similarities using Laplacian Regularization.  To
demonstrate the benefits of our approach, we consider the problem of
parsing Chinese treebank data using only lexical features, that is,
without part-of-speech tags or grammatical categories.  We achieve state
of the art performance, improving upon current large margin approaches.<p>
Here is the link for the paper:
  http://www.cs.ualberta.ca/~wqin/papers/depar_margin_conll06.pdf<p>
About the speaker:<p>
Qin Iris Wang is a Ph.D. student from the University of Alberta,
working with Dekang Lin and Dale Schuurmans. Her research interests
are in natural language processing and machine learning. Specifically,
she has been working on dependency parsing using both generative and
discriminative methods.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Jul 06
  </td><td align=left valign=top>
    Dragos Munteanu + Joseph Turian
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jul_11');">
    Practice Talks for ACL
    </a><br>
  <span id=abs06_Jul_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Extracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora
Dragos Munteanu<p>
We present a novel method for extracting parallel sub-sentential fragments
from comparable bilingual corpora. Currently, the state of the art in
comparable corpus mining is only able to extract full sentence pairs which
are judged to be parallel. We advance the state of the art by showing how
to obtain useful data even from not-fully-parallel sentences. By analyzing
sentence pairs using a signal-processing-inspired approach, we detect
which segments of the source sentence are translated into segments of the
target sentence, and which are not. We evaluate the quality of the
extracted data by showing that it improves the performance of a
state-of-othe-art machine translation system.<p>

Advances in Discriminative Parsing
Joseph Turian<p>
The present work advances the accuracy and training speed of
discriminative parsing. Our discriminative parsing method has no
generative component, yet surpasses a generative baseline on constituent
parsing, and does so with minimal linguistic cleverness. Our model can
incorporate arbitrary features of the input and parse state, and performs
feature selection incrementally over an exponential feature space during
training. We demonstrate the flexibility of our approach by testing it
with several parsing strategies and various feature sets.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 Jun 06
  </td><td align=left valign=top>
    David Chiang and Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jun_30');">
    Synchronous Grammars and Tree Transducers
    </a><br>
  <span id=abs06_Jun_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:00 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(Practice tutorial for ACL/COLING 2006)<p>
Once upon a time, synchronous grammars and tree transducers were esoteric
topics in formal language theory, far removed from the practice of
building real, large-scale natural language systems. However, these tools
are now rapidly becoming essential for modeling machine translation and
other complex language transformations. It has therefore become practical
and important to understand the basic properties of tree transformation
systems, which we cover in this tutorial.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Jun 06
  </td><td align=left valign=top>
    Joseph Turian (NYU)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jun_23');">
    Discriminative Training for Large-Scale NLP
    </a><br>
  <span id=abs06_Jun_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Parsing and translating natural languages can be viewed as
structured-prediction problems. We outline the crucial design
decisions that must be made to build a machine to solve structured
prediction problems, and explain our particular choices for these two
large-scale NLP problems.  Our approach uses a purely discriminative
learning method that scales up well to problems of this size.  Unlike
currently popular methods, this one does not require a great deal of
feature engineering a priori, because it performs feature selection
over a compound feature space as it learns.  Accuracy on constituent
parsing was at least as good as other comparable methods.  To our
knowledge, it is the first purely discriminative learning algorithm
for translation with tree-structured models.  Experiments demonstrate
the method's versatility, accuracy, and efficiency.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    26 May 06
  </td><td align=left valign=top>
    Radu Soricut and Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_May_26');">
    Defense Practice Talks: Generation and Learning
    </a><br>
  <span id=abs06_May_26 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
These are two practice talks for our upcoming thesis defenses.  The titles 
and abstracts are:<p>
--------------------------------------------------------------------------<p>
NATURAL LANGUAGE GENERATION FOR TEXT-TO-TEXT APPLICATIONS USING AN INFORMATION-SLIM REPRESENTATION<p>
Radu Soricut<p>
In this talk, I describe a new natural language generation paradigm, based
on direct transformation of textual information into well-formed textual
output.  I support this language generation paradigm with theoretical
contributions in the field of formal languages, new algorithms, empirical
results, and software implementations. At the core of this work is a novel
representation formalism for probability distributions over finite
languages. Due to its convenient representation and computational
properties, this formalism supports a wide range of language generation
needs, from sentence realization to text planning.<p>
Based on this formalism, I describe, implement, and analyze theoretically
a family of algorithms that perform language generation using direct
transformations of text. These algorithms use stochastic models of
language to drive the generation process. I perform extensive empirical
evaluations using my implementation of these algorithms. These evaluations
show state-of-the-art performance in automatic translation, and
significant improvements in state-of-the-art performance in abstractive
headline generation and coherent discourse generation.<p>

--------------------------------------------------------------------------<p>
PRACTICAL STRUCTURED LEARNING FOR NATURAL LANGUAGE PROCESSING<p>
Hal Daume III<p>
Natural language processing is replete with problems whose outputs are
highly complex and structured.  The current state-of-the-art in machine
learning is not yet sufficiently general to be applied to general problems
in NLP.  In this thesis, I present Searn (for "search" + "learn"), an
approach to learning for structured outputs that is applicable to the wide
variety of problems encountered in natural language.  Searn operates by
transforming structured prediction problems into a collection of
classification problems, to which any standard binary classifier may be
applied.  From a theoretical perspective, Searn satisfies a strong
fundamental performance guarantee: given a good classification algorithm,
Searn yields a good structured prediction algorithm.  To demonstrate
Searn's general applicability, I present applications in such diverse
areas as automatic document summarization and entity detection and
tracking.  In these applications, Searn is empirically shown to achieve
state-of-the-art performance.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 May 06
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_May_24');">
    Beyond EM: Bayesian Techniques for Human Language Technology Researchers
    </a><br>
  <span id=abs06_May_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 9:00 am - 12:00 pm<br>
<b>Location:</b> 4th Floor<br>
<b>Abstract:</b> <br>
This is a practice tutorial for one I am giving at HLT/NAACL one week
later.  Comments/feedback are very welcome.<p>
----------------------------------------------------------------------<p>
Expectation Maximization (EM) has proved to be a great and useful
technique for unsupervised learning problems in speech and language
processing.  Unfortunately, its range of applications is limited either by
intractable E- or M-steps, or by its reliance on the maximum likelihood
estimator.  The natural language processing community typically resorts to
ad-hoc approximation methods to get (some reduced form of) EM to apply to
NLP tasks.  However, many of the problems that plague EM can be solved
with Bayesian methods, which are theoretically more well justified.  In
this tutorial, I discuss Bayesian methods as they can be used in natural
language processing.  The two primary foci of this tutorial are specifying
prior distributions and performing the necessary computations to perform
inference in Bayesian models.  I focus on unsupervised techniques (for
which EM is the obvious choice), but discuss supervised and discriminative
techniques at the conclusion with pointers to relevant literature.<p>
Depending on one's inference technique of choice, the math required to
build Bayesian learning models can be difficult.  Compounding this problem
is the fact that current written tutorials on Bayesian techniques tend to
focus on continuous-valued problems, a poor match for the high-dimension
discrete world of text.  This combination makes the cost of entrance to
the Bayesian learning literature often too high.  The goal of this
tutorial is to provide sufficient motivation, intuition and vocabulary
mapping so that one can easily understand recent papers in Bayesian
learning that are published at conferences like NIPS, and increasingly at
ACL.  In addition to the standard tutorial materials (slides), this
tutorial is accompanied by a technical report that spells out all the
mathematic derivations in great detail, for those who wish to start
research projects in this fields.<p>
This tutorial should be accessible to anyone with a basic understanding of
statistics.  I use a query-focused summarization task as a motivating
running example for the tutorial, which should be of interest to
researchers in natural language processing and in information retrieval.  
Additionally, though the tutorial does not focus on speech problems, those
attendees interested in graphical modeling techniques for automatic speech
recognition might also find the tutorial of interest.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    19 May 06
  </td><td align=left valign=top>
    Patrick Pantel
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_May_19');">
    Espresso: Making Use of Generic Patterns for Mining Relations from Small and Large Corpora
    </a><br>
  <span id=abs06_May_19 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In the past decade, researchers have explored many approaches to
automatically extract large collections of knowledge from text. In this
talk, we present Espresso, a weakly-supervised, general-purpose, and
broad-coverage algorithm for harvesting binary semantic relations. The
main contributions are: i) a method for exploiting generic patterns by
filtering incorrect instances using the Web; and ii) a principled measure
of pattern and instance reliability enabling the filtering algorithm. We
present an empirical comparison of Espresso with various state of the art
systems, on different size and genre corpora, on extracting various
general and specific relations. Experimental results show that our
exploitation of generic patterns substantially increases system recall
with small effect on overall precision.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 May 06
  </td><td align=left valign=top>
    Nick Mote and Donghui Feng
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_May_12');">
    Pedagogical Contextualization of Language Learner Speech Errors AND Learning to Detect Conversation Focus of Threaded Discussions
    </a><br>
  <span id=abs06_May_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This is two practice talks.<p>
-----------------------------------------------------------------------------
FIRST TALK:<p>
The traditional approach to diagnosing learner speech errors in Computer
Aided Language Learning is to create a linguistic profile of the
learner/user. We, however, propose that work must also be done to model
the linguistic profile of a typcial native listener.<p>
Not all errors in second langage learner speech are created equal.
Different errors sound more "severe" or "harsh" to native speaker ears and
should therefore be treated with more emphasis in pedagogical interaction.<p>
The Tactical Language Training System (TLTS) is a speech-enabled
virtual-reality based computer learning environment designed to teach
Arabic spoken communication to American English speakers. This talk
addresses the ways the TLTS contextualizes non-native speech errors, and
how this contextualization fits in the corrective exchanges between a
non-native learner and a pedagogical agent built to model a native
listener.<p>
The pedagogical system used in TLTS includes:<p>
 * Automatic Speech Recognition (ASR) models which are built on a
    combination of both annnotated and unannotated non-native speech with
    native speech data.<p>
 * A stochastic generative model for errors in learner speech that
    creates mispronunciation grammars for the ASR<p>
 * Reweighting of system-perceived mispronunciation severity based on
    aggregate native speaker judgements of quality pronunciation and
    intelligiblity.<p>
 * Contextualization of feedback based on lexical and phonetic
    inventories of the native and non-native languages.<p>

-----------------------------------------------------------------------------
SECOND TALK:<p>
We present a novel feature-enriched approach that learns to detect the
conversation focus of threaded discussions by combining NLP analysis and
IR techniques. Using the graph-based algorithm HITS, we integrate
different features such as lexical similarity, poster trustworthiness, and
speech act analysis of human conversations with featureoriented link
generation functions. It is the first quantitative study to analyze human
conversation focus in the context of online discussions that takes into
account heterogeneous sources of evidence. Experimental results using a
threaded discussion corpus from an undergraduate class show that it
achieves significant performance improvements compared with the baseline
system.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 May 06
  </td><td align=left valign=top>
    Namhee Kwon
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_May_05');">
    Recognizing Argument Structures in Texts
    </a><br>
  <span id=abs06_May_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I present our approach to identify an argument structure defined as a
simple hierarchical structure of claim and reasons.  The claim is also
classified into "in favor of" or "against" the topic. The experiment is
performed on the comments from the general public sent to government
officials in response to proposed regulations.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    28 Apr 06
  </td><td align=left valign=top>
    Feng Pan
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Apr_28');">
    Learning Event Durations from Event Descriptions
    </a><br>
  <span id=abs06_Apr_28 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The research of extracting event duration information from texts is 
potentially very important in applications in which the time course of 
events is to be extracted from news.  For example, whether two events 
overlap or are in sequence often depends very much on their durations.  If 
a war started yesterday, we can be pretty sure it is still going on today. 
If a hurricane started last year, we can be sure it is over by now.<p>
In the talk, I will first present our work on constructing an annotated 
corpus for extracting information about the typical durations of events 
from texts, including the annotation guidelines, the event classes we 
categorized, the way we use normal distributions to model such vague and 
implicit temporal information, and how we evaluate inter-annotator 
agreement. I will then show that machine learning techniques applied to 
this data yield coarse-grained event duration information, considerably 
outperforming a baseline and approaching human performance.<p>
At the beginning of the talk, I will also give a brief overview of the 
time ontology (OWL-Time, formerly DAML-Time) we have developed, which is 
represented in both first-order logic and the OWL web ontology language.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    21 Apr 06
  </td><td align=left valign=top>
    Soo-Min Kim
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Apr_21');">
    Identifying and Analyzing Judgment Opinions
    </a><br>
  <span id=abs06_Apr_21 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this talk, we introduce a methodology for analyzing judgment opinions.
We define a judgment opinion as consisting of a valence, a holder, and a
topic. We decompose the task of opinion analysis into four parts: 1)
recognizing the opinion; 2) identifying the valence; 3)  identifying the
holder; and 4) identifying the topic. We evaluate our methodology using
both intrinsic and extrinsic measures.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    14 Apr 06
  </td><td align=left valign=top>
    Radu Soricut
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Apr_14');">
    Natural Language Generation for Text-to-Text Applications using an Information-Slim Representation
    </a><br>
  <span id=abs06_Apr_14 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Although a considerable number of generic Natural Language Generation
(NLG) systems has been produced over the years, none of them is usually
employed in end-to-end, text-to-text applications such as Machine
Translation, Summarization, Question Answering, etc. In this talk, we
identify the likely reasons for this state of affairs, and propose
WIDL-expressions as a flexible formalism that facilitates the integration
of a generic NLG engine within end-to-end language processing
applications.
 
WIDL-expressions represent compactly probability distributions over finite
sets of candidate realizations, and have optimal algorithms for text
realization via interpolation with language model probability
distributions. We show the effectiveness of our WIDL-based NLG engine for
both sentence realization and document realization tasks. By employing
language models that capture sentence-level properties, we perform Machine
Translation and Headline Generation at state-of-the-art levels or better.
By employing language models that capture document-level properties such
as text coherence, we synthesize output for Multi-document Summarization
that displays both high content selection performance and increased
coherence.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Mar 06
  </td><td align=left valign=top>
    Dragos Munteanu
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Mar_24');">
    Automatic creation of parallel corpora
    </a><br>
  <span id=abs06_Mar_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Parallel texts -- texts that are translations of each other -- are an
important resource in many cross-lingual NLP applications, such as lexical
acquisition, cross-language IR, and annotation projection. However, their
importance is paramount for Statistical Machine Translation (SMT), as they
provide the training data from which all the translation knowledge is
learned. The state of the art in SMT is advanced enough that, given
sufficient parallel data (i.e. a few million words) for any language pair
in a given domain, a generic SMT system trained on it will achieve a
reasonable translation performance in that domain. The main reason why SMT
systems exist only for a handful of languages is that, for most language
pairs, parallel training data is simply not available.<p>
One way to alleviate this lack of parallel data is to exploit a much
richer and more diverse resource: comparable corpora, texts which are not
strictly parallel but related. The prototypical example of comparable
texts are two news articles in different languages which report on the
same event. I will present methods for automatic extraction of parallel
data from such corpora. I will show how to detect parallel data at various
levels of granularity: parallel documents, parallel sentences, and even
parallel sub-sentence fragments. The parallel corpora obtained using these
methods help improve translation performance for both resource-scarce
language pairs (such as Romanian-English) and resource-rich ones (such as
Arabic-English).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Mar 06
  </td><td align=left valign=top>
    Jon May
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Mar_17');">
    Tiburon: A Finite State Tree Automata Toolkit
    </a><br>
  <span id=abs06_Mar_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4th Floor<br>
<b>Abstract:</b> <br>
In the 1990s, researchers applied their new developments in transducer
theory using widely available easy-to-use toolkits for string transducers,
and made well-known advances in parsing, machine translation, and other
areas. Rapid prototyping via software such as the AT&T toolkit and carmel
was useful for proofs of concept and in many cases led to unforseen
developments in novel areas. In the current nlp research environment tree
based strategies and new models have shown promising results in advancing
the state of the art, and recent developments in weighted tree automata
theory are enriching the bedrock created 40 years ago, but as of yet there
is no toolkit available with the necessary capabilities to turn promise
into solution.<p>
Tiburon is the first probablistic tree transducer toolkit. Similar in form
and function to the string-based toolkits of yesteryear, it is designed to
be easy to use, with simple but expressive definitions of tree automata
and a concise set of vital operations that can be used to construct many
useful tree-based nlp projects. Although a work in progress, Tiburon is
already a usable tool with active users between the ages of 6 and 41. I
will describe the current status of the system, demonstrate ease of use
and potential power, and discuss the challenges ahead.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Mar 06
  </td><td align=left valign=top>
    Mark Hopkins
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Mar_10');">
    Exploring the Potential of Intractable Parsers
    </a><br>
  <span id=abs06_Mar_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 10th Floor<br>
<b>Abstract:</b> <br>
We revisit the idea of history-based parsing, and present a history-based
parsing framework that strives to be simple, general, and flexible. We
also provide a decoder for this probability model that is linear-space,
optimal, and anytime. A parser based on this framework, when evaluated on
Section 23 of the Penn Treebank, compares favorably with other
state-of-the-art approaches, in terms of both accuracy and speed.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    03 Mar 06
  </td><td align=left valign=top>
    Liang Huang (Penn)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Mar_03');">
    Syntax-Directed Translation with Extended Domain of Locality
    </a><br>
  <span id=abs06_Mar_03 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11th Floor (Large)<br>
<b>Abstract:</b> <br>
(note: this is a very tentative title -- comments welcome!)<p>
We present a novel extension of syntax-directed translation for
statistical MT. Formally speaking, our model is based on tree-to- string
transducers that recursively convert a parse-tree in the source-language
into a string in the target-language. These transduction rules have
multi-level trees on the source-side, giving this system more
transformational power due to the extended domain of locality. We also
present efficient algorithms for decoding based on dynamic programming.
Initial experiments on English-to-Chinese translation show promising
results in both speed and the translation quality.<p>
Joint work with Kevin Knight and Aravind Joshi.<p>
Bio:<p>
Liang Huang is a 3rd-year PhD student from the University of Pennsylvania.
He is mainly interested in algorithms and formalisms for parsing and
syntax-based machine translation. His recent work has been on k-best
parsing algorithms (with David Chiang) and synchronous binarization for MT
(with Hao Zhang, Dan Gildea, and Kevin Knight).

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Feb 06
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Feb_24');">
    Search-based Structured Prediction
    </a><br>
  <span id=abs06_Feb_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I present an algorithm, Searn (for "search-learn") that is designed to
solve structured prediction problem: problems whose goal is to learn to
predict complex objects such as parts-of-speech, parse trees,
translations, etc...  Searn functions by "breaking apart" structured
prediction problems into classification problems in the process of search.  
I analyze Searn in the framework of learning reductions and show that good
performance on the underlying classification problems implies good search
performance.  Moreover, Searn is computationally efficient in a superset
of the settings where previous algorithms are efficient and is not limited
by conditional independence assumptions (as in CRFs).  This excessively
simple and general algorithm turns out to have excellent state-of-the-art
performance.<p>
This is joint work with John Langford (TTI-C) and Daniel Marcu; and, to a
lesser extent, with Drew Bagnell (CMU) and Bianca Zadrozny (IBM TJ
Watson).

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Feb 06
  </td><td align=left valign=top>
    David Chiang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Feb_10');">
    Parsing Arabic Dialects
    </a><br>
  <span id=abs06_Feb_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The Arabic language exhibits diglossia, i.e., the coexistence of two forms
of language, a variety with standard orthography and sociopolitical clout
which is not natively spoken by anyone (Modern Standard Arabic, MSA) and
varieties that are primarily spoken and lack writing standards (Arabic
dialects). There are important resources currently available for MSA with
much on-going NLP work; for example, there is an Arabic Treebank and
several syntactic parsers for MSA.  However, Arabic dialect resources and
NLP research are still at an infancy stage. I will present work done at
the Johns Hopkins CLSP Summer Workshop on parsing of Arabic dialects, in
particular, Levantine Arabic.  We have experimented with three approaches
to leveraging MSA resources to create a parser for Levantine Arabic, as
well as methods for induction of MSA-Levantine translation lexicons and a
Levantine part-of-speech tagger. Using these methods we obtain error
reductions of up to 15% compared with applying an MSA parser directly to
Levantine text.<p>
Rambow et al. Parsing Arabic Dialects: Final Report. Johns Hopkins
University Center for Language and Speech Processing Workshop 2005.  
http://www.clsp.jhu.edu/ws2005/groups/arabic/documents/finalreport.pdf<p>
Chiang et al. Parsing Arabic Dialects. To appear in Proc. EACL 2006.<p>
This is joint work with O. Rambow, M. Diab, N. Habash, R. Hwa, K. Sima'an,
V.  Lacey, R. Levy, C. Nichols and S. Shareef.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    03 Feb 06
  </td><td align=left valign=top>
    Alex Fraser
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Feb_03');">
    Measuring Word Alignment Quality for Statistical Machine Translation
    </a><br>
  <span id=abs06_Feb_03 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Automatic word alignment plays a critical role in statistical machine
translation. Unfortunately the relationship between alignment quality and
statistical machine translation performance has not been well understood.
In the recent literature the alignment task has frequently been decoupled
from the translation task, and assumptions have been made about measuring
alignment quality for machine translation which, it turns out, are not
justified. In particular, none of the tens of papers published over the
last five years has shown that significant decreases in Alignment Error
Rate (AER) result in significant increases in translation quality. I will
explain this state of affairs and present steps towards measuring
alignment quality in a way which is predictive of statistical machine
translation quality.<p>
I will also provide a brief overview of some of my other work on training
and search for word alignment.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 Jan 06
  </td><td align=left valign=top>
    John Conroy
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jan_27');">
    Multi-Document Summary Space:What do People Agree is Important?
    </a><br>
  <span id=abs06_Jan_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
A multi-document summary gives the "gist" of what is contained in a
collection of related documents. But how can we define a "gist?" We
explore this question by analyzing human written summaries for clusters of
document sets. In particular, we estimate the probability that word will
be chosen by a human to be included in a summary. We demonstrate that if
this probability model were given by an oracle, then a simple automatic
method of summarization can produce extract summaries which are
statistically indistinguishable from the human summaries.<p>
About the Speaker:<p>
John M. Conroy received a B.S. in Mathematics from Saint Joseph's
University in 1980 and a Ph.D. in Applied Mathematics from the University
of Maryland in 1986. Since then he has been a research staff member for
the IDA Center for Computing Sciences in Bowie, MD. His research interest
is applications of numerical linear algebra and statistics. He is a member
of the Society for Industrial and Applied Mathematics, Institute of
Electrical and Electronics Engineers (IEEE), and the Association for
Computational Linguistics.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    26 Jan 06
  </td><td align=left valign=top>
    Tim Chklovski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs06_Jan_26');">
    GrainPile: Deriving Quantitative Overviews of Free Text Assessments on the Web
    </a><br>
  <span id=abs06_Jan_26 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 1:00 pm - 2:00 pm<br>
<b>Location:</b> 4th floor<br>
<b>Abstract:</b> <br>
Many research efforts are addressing the problem of enabling automatic
summarization of opinions and assessments stated on the web in product
reviews, discussion forums, and blogs. One key difficulty is that relevant
assessments scattered throughout web pages are obscured by variations in
natural language. In this paper, we focus on a novel aspect of enabling
aggregations of assessments of degree to which a given property holds for
a given entity (for instance, how touristy is Boston). We present
GrainPile, a user interface for extracting from the web, aggregating and
quantifying degree assessments of unconstrained topics. The interface
provides a variety of functions: a) identification of dimensions of
comparison (properties) relevant to a particular entity or set of
entities, b) comparisons of like entities on user-specified properties
(for example, which university is more prestigious, Yale or Cornell), c)
tracing the derived opinions back to their sources (so that the reasons
for the opinions can be found). A central contribution in GrainPile is the
evaluated demonstration of feasibility of mapping the recognized
expressions (such as fairly, very, extremely, and so on) to a common scale
of numerical values and aggregating across all the extracted assessments
to derive an overall assessment of degree. GrainPile&#8217;s novel
assessment and aggregation of degree expressions is shown to strongly
outperform an interpretation-free, co-occurrence based method.<p>
Full paper:<p>
http://www.isi.edu/~timc/papers/IUI06-grainpile-chkl.pdf<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Dec 05
  </td><td align=left valign=top>
    Jonathan May
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Dec_16');">
    A Better N-Best List - Practical Determinization of Weighted Finite Tree Automata
    </a><br>
  <span id=abs05_Dec_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Ranked lists of output trees from syntactic statistical NLP applications
frequently contain multiple repeated entries. This redundancy leads to
misrepresentation of tree weight and reduced information for debugging and
tuning purposes. It is chiefly due to nondeterminism in the weighted
automata that produce the results. I will introduce an algorithm that
determinizes such automata while preserving proper weights, returning the
sum of the weight of all multiply derived trees. I will also report
results of the application of the algorithm to machine translation and
Data Oriented Parsing.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 Sep 05
  </td><td align=left valign=top>
    David Chiang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Sep_30');">
    Some Computational Complexity Results for Synchronous Context-Free Grammars
    </a><br>
  <span id=abs05_Sep_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4 Large<br>
<b>Abstract:</b> <br>
(This is a practice talk for a paper by Giorgio Satta and Enoch Peserico)<p>
This paper investigates some computational problems associated with
probabilistic translation models that have recently been adopted in the
literature on machine translation. These models can be viewed as pairs of
probabilistic context-free grammars working in a `synchronous' way. Two
hardness results for the class NP are reported, along with an exponential
time lower-bound for certain classes of algorithms that are currently used
in the literature.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    29 Sep 05
  </td><td align=left valign=top>
    Tim Chklovski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Sep_29');">
    Previews of my talks for K-CAP
    </a><br>
  <span id=abs05_Sep_29 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

The topics & approximate start times:<p>
(3:00 sharp) My 7-10 min bit for panel discussion on "Manual vs. Automated
Knowledge Acquisition"<p>
Will touch on web extraction vs. learning from volunteers -- strengths and
weaknesses, new thoughts on synergies<p>
(3:15) Designing Intelligent Acquisition Interfaces for Collecting World
Knowledge from Web Contributors
(paper by Timothy Chklovski, Yolanda Gil)<p>
(3:55) Collecting Paraphrase Corpora from Volunteer Contributors (paper by
Timothy Chklovski)

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    26 Aug 05
  </td><td align=left valign=top>
    Fossum, Huang and Zhang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Aug_26');">
    Summer Student Presentations
    </a><br>
  <span id=abs05_Aug_26 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
3:00pm  Victoria Fossum (Michigan)
Exploring the Continuum between Phrase-based and Syntax-based Machine Translation<p>
State-of-the-art statistical machine translation systems use lexical
phrases as the basic unit of translation.  Phrase-based systems can
capture those aspects of translation that are sensitive to local context.  
Syntax-based systems, on the other hand, make use of linguistically
motivated syntactic structure, can capture long-distance dependencies and
reorderings, and offer greater generalization in translation rules.  
However, their performance lags that of phrase-based systems.<p>
Hierarchical phrase-based translation, introduced by [Chiang 05], provides
an elegant framework for exploring the continuum between phrase-based and
syntax-based translation.  This system combines the "formal machinery" of
syntax-based systems without any "linguistic commitment" to a particular
syntactic structure [Chiang 05].<p>
I will present results from my re-implementation of Chiang's hierarchical
phrase-based system, and (if time permits) compare those results with the
following systems on Chinese-English translation: ISI's phrase-based
system, and ISI's syntax-based system.  Between now and December 2005, I
plan to incrementally explore the space between phrase-based and
syntax-based systems by augmenting these hierarchical phrase-based rules
with richer syntactic annotation.<p>

3:30pm  Liang Huang (Penn) and Hao Zhang (Rochester)
Efficient Integration of n-gram Language Models with Syntax-based Decoding<p>
We first give an overview of the ISI syntax-based MT system which is based
on tree-to-string (xRs) translation rules. The biggest problem at this
stage is the inefficiency of the integration of n-gram models.  Without
n-gram models, the xRs translation rules can be easily binarized with
respect to the foreign language to ensure cubic-time decoding. With n-gram
models, however, binarization without considering both languages will lead
to exponential complexity.<p>
Inspired by Inversion Transduction Grammar (ITG) (Wu, 97), we will focus
on the so-called ITG binarizable rules which count for over 99% of the
whole rule set. A simple linear-time algorithm will be presented to do the
binarization. Decoding with ITG-like rules is of low polynomial complexity
in both time and space. We will discuss experimental results on both
efficiency and accuracy of decoding with the new binarization.  If time
permits, we will also present the "hook trick" (inspired by (Eisner and
Satta, 99)) to even further reduce the polynomial complexity of the
decoding process.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Aug 05
  </td><td align=left valign=top>
    Hopkins, Reisa, and Nakov
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Aug_24');">
    Summer Student Presentations
    </a><br>
  <span id=abs05_Aug_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:30 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
3:30pm  Mark Hopkins (UCLA)
Tree Sequence Automata: A Unifying Framework for Tree Relation Formalisms<p>
There exist a wide variety of competing formalisms for representing a
language of ordered tree pairs.  These include (bottom-up and top-down)  
tree transducers, synchronous tree-substitution grammars (STSGs),
synchronous tree-adjoining grammars (STAGs), and inversion transduction
grammars (ITGs).  Since these formalisms have all developed independently
of one another, it is difficult to compare their respective
representational power.  This work seeks to make this task simpler by
viewing these formalisms as instances of a general unifying formalism,
which we call tree sequence automata (TSA).  By casting these different
formalisms in a single framework, we can compare them directly by studying
the specific subclass of TSA that they fall into.<p>
4:00pm  Jason Riesa (Johns Hopkins)
A case study in building a cost-effective speech-to-speech machine translation system with sparse resources: English - Iraqi Arabic<p>
The Arabic spoken dialect of Iraq is a language deprived of the vast
resources that researchers enjoy when working with its written
counterpart, Modern Standard Arabic (MSA). The Iraqi Arabic lexicon and
grammar are also sufficiently distinct so that the use of existing tools
or corpora for MSA yield little or no positive effect on machine
translation output quality.  One can see that building a machine
translation system normally dependent on a large parallel corpus is a
particularly difficult task when given just a 37,000 line translated
parallel text based on transcribed speech. This talk will explore the
constraints involved in working with this type of data, how we endeavored
to mitigate such problems as a non-standard orthography and a highly
inflected grammar, and propose a cost- effective way for dealing with such
projects in the future.<p>
4:30pm  Preslav Nakov (UC Berkeley)
Multilingual Word Alignment<p>
Recently there has been a growing number of available multilingual
parallel texts. One such source is the European Union, which publishes its
official documents in the official languages of all member states
(sometimes also in the languages of the candidates). Another source are
the United Nations. These corpora are a great source of training data for
machine translation between new language pairs. But they also offer the
opportunity to obtain better pairwise word alignments by looking at
multiple languages in parallel. In this talk I will present my research as
a summer intern at ISI on getting better French (Fr) to English (En) word
alignments using an additional language (Xx). First, I will introduce two
heuristics which start with pairwise alignments between Fr-Xx, En-Xx and
Fr-En and then combine them probabilistically (in a linear model) or
graph-theoretically (by looking at in- and out-degrees for each word).  
Then I will present two Model1 inspired alignment models: (a) from "Fr and
Xx" to En; and (b) from Fr to "En and Xx".

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Aug 05
  </td><td align=left valign=top>
    Doug Oard (Maryland)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Aug_05');">
    The CLEF Cross-Language Speech Retrieval Test Collection
    </a><br>
  <span id=abs05_Aug_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Test collections for information retrieval tasks have traditionally
assumed that what we are searching for are documents (e.g., Web pages,
news stories, or academic documents).  Most information that is generated
is, however, not in originally generated as part of a document, but rather
as what we might refer to as "conversational media" (e.g., email, speech,
or instant messaging).  In this talk, I'll describe the creation of two
test collections for conversational media, an email collection being
created in the TREC Enterprise Search track and a spoken word test
collection for the the Cross-Language Evaluation Forum (CLEF).  I'll spend
most of the talk describing the details of the CLEF test collection,
illustrating the issues with some of the results that we have obtained
from our experiments with that collection.  I'll conclude with a few
remarks about the implications of what we are learning for DARPA's new
GALE program.  This is joint work with Charles University, the IBM TJ
Watson Research Center, the Johns Hopkins University, the Survivors of the
Shoah Visual History Foundation, and the University of West Bohemia.<p>

About the speaker:<p>
Douglas Oard is an Associate Professor at the University of Maryland,
College Park, with a joint appointment in the College of Information
Studies and the Institute for Advanced Computer Studies.  He holds a Ph.D.
in Electrical Engineering from the University of Maryland, and his
research interests center around the use of emerging technologies to
support information seeking by end users.  In 2002 and 2003, Doug spent a
year in paradise here at USC-ISI.  His recent work has focused on
interactive techniques for cross-language information retrieval and on
searching conversational text and speech.  Additional information is
available at http://www.glue.umd.edu/~oard/.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Aug 05
  </td><td align=left valign=top>
    Jan Hajic (Charles U)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Aug_05b');">
    The Family of Prague Dependency Treebanks
    </a><br>
  <span id=abs05_Aug_05b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The Prague Dependency Treebank project is aimed at a linguistically
complex, multi-tier annotation of relatively large amounts of naturally
occuring sentences of natural language. There are four tiers at present:
the basic token tier (level 0), and the morphological, surface-syntacic,
and semantic (called "tectogrammatics") tiers. The syntactic and
tectogrammatic tiers are based on a richly labelled dependency
representation principle. So far, the project produced three corpora: the
Czech-language-only Prague Dependency Treebank, the Prague Czech-English
Dependency Treebank and the Prague Arabic Dependency Treebank. In the
talk, the principles of the Prague Dependency Treebank linguistic
annotation scheme will be presented. Some technical details will also be
discussed, as well as some of the tools developed both for the manual
annotation itself and for corpus-based NLP of Czech, English and Arabic.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Jul 05
  </td><td align=left valign=top>
    Victoria Li Fossum (Michigan)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jul_15');">
    Inducing POS Taggers by Projecting from Multiple Source Languages
    </a><br>
  <span id=abs05_Jul_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(Yarowsky et al., 2001) present an algorithm for bootstrapping a POS
tagger for an arbitrary target language, using an existing POS tagger for
a source language and a parallel corpus in the source and target
languages.  The source text is annotated with the POS tagger; the parallel
corpus is word-aligned; the POS tags are "projected" from source to target
language; and finally smoothing is performed before training a POS tagger
for the target language on the projected annotations.<p>
I will talk about my work (jointly with my advisor, Steve Abney, at U. of
Michigan) in which we extend this algorithm by projecting from multiple
source languages onto a target language, then combining the outputs to
compute a consensus POS tagger.  Our hypothesis is that systematic
transfer errors from different source-target pairs can be reduced by using
multiple source languages.  I will present experimental results for three
different source languages (English, German, and Spanish), and two
different target languages (French and Czech).  Our results indicate that
using multiple source languages improves performance.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    07 Jul 05
  </td><td align=left valign=top>
    Radu Soricut
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jul_07');">
    Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation
    </a><br>
  <span id=abs05_Jul_07 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>
Text-to-text applications -- Machine Translation, Summarization, Question
Answering -- do not usually involve generic Natural Language Generation
(NLG) systems in their generation components, but rather use
application-specific algorithms. The main reason for this state of affairs
is that virtually all the formalisms used by current generic NLG systems
require information that cannot be reliably extracted from unrestricted
text.<p>
This thesis proposal is about meeting the demand for natural language
generation in the context of text-to-text applications. I introduce a new
representation formalism (WIDL-expressions), propose generation algorithms
that operate on representations specific to this formalism, and discuss a
generic sentence realization framework for text-to-text applications. The
generation mechanism is based on algorithms for intersecting
WIDL-expressions with probabilistic language models. I present both
theoretical and empirical results concerning the correctness and
efficiency of these algorithms. I also discuss the practical aspects
arising from implementing this generation mechanism.<p>
In a concrete application of the proposed generation mechanisms, I present
an end-to-end Machine Translation application. I also discuss another
possible application for Automated Summarization, namely automated
headline generation.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    06 Jul 05
  </td><td align=left valign=top>
    Alessandro Moschitti (Rome)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jul_06');">
    Kernel Methods for Semantic Role Labeling
    </a><br>
  <span id=abs05_Jul_06 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:00 pm - 3:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Automatic Natural Language applications often require the processing of
structured data. Traditional machine learning approaches attempt to
represent structured syntactic/semantic objects by means of flat feature
representations, i.e. attribute-value vectors. This raises two problems:<p>
1. There is no well defined theoretical motivation for such feature model.
Structural properties may not fit in any flat feature representation.<p>
2. To define effective flat features, a deep knowledge about the
linguistic phenomenon is required.<p>
Kernel methods for Natural Language Processing aim to solve both the above
problems as kernel functions can be used to define similarities between
linguistic objects without explicitly defining the target feature space.  
In this way, a linguistic phenomenon can be modeled at a more abstract
level where the modeling is easier. Such property is extremely useful when
the representation of linguistic phenomena is still not well understood.
For example, the feature design of semantic role labeling appear to be
quite complex since several and non-definitive feature sets have been
proposed.<p>
As a viable alternative to manual feature design, kernel methods propose
two steps: (1) they generate all substructures of the target
syntactic/semantic structures and (2) they let the learning algorithm
(e.g. Support Vector Machines) to select the most relevant substructures.
In this talk, we (1) introduce the PropBank and FrameNet predicate
argument structures, (2) present the standard approaches to the automatic
labeling of semantic roles and (3) show advanced semantic role labeling
models based on kernel methods.<p>
About the speaker:<p>
Alessandro Moschitti is a researcher at the Computer Science Department of
the University of Rome ^Tor Vergata^. In 1998 he took his master degree
in Computer Science at the University of Rome ^La Sapienza^. In 2003 he
finished his PhD in Computer Science at ^Tor Vergata^ University.  
Between 2002 and 2004 he worked as an associate researcher in the
University of Texas at Dallas. His research interests concern machine
learning approaches for Natural Language Processing and Information
Retrieval. His deep expertise relates to automated text categorization and
semantic role labeling.  Recently, he has devised new kernels which enable
Support Vector and other kernel-based machines to carry out advanced
semantic processing.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Jun 05
  </td><td align=left valign=top>
    Michael Fleischman (MIT)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_23');">
    Intentional Context in Situated Language Learning
    </a><br>
  <span id=abs05_Jun_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>
Natural language interfaces designed for agents that interact with users
in shared environments (e.g. training simulators, videogames) must
incorporate knowledge about the users' context in order to address the
many ambiguities of situated language use. We introduce a model of
situated language acquisition that operates in two phases.  First,
intentional context is represented and inferred from user actions using
probabilistic context free grammars.  Then, utterances are mapped onto
this representation in a noisy channel framework.  The acquisition model
is trained on unconstrained speech collected from subjects playing an
interactive game, and tested using an understanding task.  Discussion of
results focuses both on the implications for theoretical models of
cognition, as well as, for natural language applications in shared
environments.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Jun 05
  </td><td align=left valign=top>
    Mitsunori Matsushita
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_22b');">
    Lumisight Table: A Face-to-face Collaboration Support System That Optimizes Direction of Projected Information to Each Stakeholder
    </a><br>
  <span id=abs05_Jun_22b style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 11:00 am - 12:00 pm (Wednesday the 22nd!)<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(This talk occurs in the morning on the same day as the Bayesian tutorial.)<p>
The goal of our research is to support cooperative work performed by
stakeholders sitting around a table. To support such cooperation, various
table-based systems with a shared electronic display on the tabletop have
been developed. These systems, however, suffer the common problem of not
recognizing shared information such as text and images equally because the
orientation of their view angle is not favorable. To solve this problem,
we propose the Lumisight Table. This is a system capable of displaying
personalized information to each required direction on one horizontal
screen simultaneously by multiplexing them and of capturing stakeholders'
gestures to manipulate the information.<p>
About the Speaker:<p>
Mitsunori Matsushita is a research scientist of NTT Communication Science
Labs., Nippon Telegraph and Telephone Corporation (NTT). He received B.E.,
M.E., and Dr.E. degrees from Osaka University, in 1993, 1995 and 2003
respectively. In 1995, he joined NTT, and has been engaged in researches
on natural language understanding, information visualization, and
interaction design.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Jun 05
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_22');">
    Beyond EM: Bayesian Techniques for NLP Researchers
    </a><br>
  <span id=abs05_Jun_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 1:00 pm - 4:30 pm (Wednesday and long!)<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
EM has proved to be a great and useful technique for unsupervised learning
problems in natural language.  Unfortunately, it cannot solve every
problem out there, either because the E-step is intractable, the M-step is
intractable or both.  Typically our community resorts to a Viterbi
approximation in this case, which really isn't very justified and can
easily diverge from our expectations (no pun intended). Moreover, EM --
like all maximum likelihood methods -- suffers from a need for ad-hoc and
undesirable smoothing.  All of these problems -- intractable E- or
M-steps, the Viterbi approximation, and the annoyance of smoothing -- are
solved by using Bayesian methods. Moreover, from a theoretic point of
view, the Bayesian paradigm is much more foundationally well justified
than the frequentist use of estimators (such as the maximum likelihood
estimator), at some cost in computation (though not as much as you might
believe).<p>
In this tutorial, I will discuss Bayesian methods as they can be used in
natural language processing.  The first half will be background (some of
which you probably won't have seen, some of which you probably will have
seen, but which will probably be presented in a different way that you're
used to) including graphical models, EM, priors and pro- (and con-)
Bayesian arguments.  The second half of the tutorial will focus on solving
complex inference problems, essentially building on what we've seen from
EM.  I'll cover MAP (*not* Bayesian -- if you can't tell me why, then you
should come to the tutorial!), summing, Monte Carlo, MCMC, Laplace,
variational and expectation propagation.  Time permitting, I will briefly
discuss Bayesian discriminative models (basically what a Bayesian uses
instead of SVMs), non-parametric (infinite) models and Bayesian decision
theory, all of which make use of the inference techniques we will have
already covered.<p>
This tutorial is intended to be largely self contained, though I will
expect that you know what probabilities are, what distributions are and
the standard manipulations of conditional/joint distributions. Familiarity
with EM would be helpful, but I'll cover this topic in some depth since it
will be important for understanding the rest of the tutorial.  I hope --
though this never really seems to come to fruition -- that this will be a
semi-interactive talk and I will attempt to adjust according to what
people are interested in and what is putting people to sleep.<p>
(see http://www.isi.edu/~hdaume/bayesnlp/ for more information)<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    20 Jun 05
  </td><td align=left valign=top>
    Birte Loenneker (Hamburg)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_20');">
    Between Story Generation and Natural Language Generation
    </a><br>
  <span id=abs05_Jun_20 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:00 am - 11:30 am<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>
Narratology analyzes the discursive structure of narratives as finalized
products of human invention, such as novels, short-stories, or
fairy-tales. Those narratives are rendered in a given surface form;
Narratology focuses on narratives in natural language. Narratologists
assume that each narrative surface representation is associated with a
neutral, abstract event sequence, the "Story" (histoire, sjuzhet). The
abstractness of Story is illustrated by the fact that the same Story can
be realized in different surface texts. By discursive structure or
"Discourse" (discours, fabula), narralogists mean the relation between an
abstract Story and its concrete expression in a sequential text. For
example, if the chronological order of the Story is not respected in its
textual recount, we are dealing with the Discourse parameter of order.
Other Discourse parameters include the frequency with which Story events
are evoked, the point of view from which they are narrated (perceived,
evaluated,...), or framed narratives with several narrative levels.<p>
The Story Generator Algorithms project at the University of Hamburg
evaluated several existing Story Generators with respect to their
discursive abilities. It became obvious that most Story Generators
concentrate on creating a coherent and chronological abstract Story,
which is directly mapped onto natural language. This results in a
predominance of 1:1 relations between Story and surface, and in most
cases corresponds to a default or zero instantiation of Discourse
parameters. As a consequence, Story Generator outputs tend to be very
explicit and straightforward, and are likely to be perceived as uniform
and boring.<p>
Narratological expert knowledge might be useful to future enhanced Story
Generators and to Natural Language Generation systems dealing with
narrative. One of the aims of Computational Narratology is to model that
expert knowledge. Ideally, narratological knowledge will be integrated
into a Narratological Structurer, as a processing component of an
advanced system that creates narratives. In such a system, the
Narratological Structurer will be the interface between a Story Generator
and subsequent Natural Language Generation modules. The talk also
presents examples of the knowledge that is being modelled.<p>

About the Speaker:<p>
Birte Lnneker graduated from the University of Hamburg, Germany, with a
degree in French with Finno-Ugristics (Finnish) and Business
Administration. Since then, her main fields of publication are Cognitive
Linguistics and electronic resources for Natural Language Processing,
with special focus on frames and metaphors, as well as electronic
dictionaries, corpora, and recently part-of-speech tagging. Her PhD on
Concept Frames and Relations, also published as a book in 2003, was
co-supervised at the Institute for Romance Languages and at the
Department of Informatics in Hamburg. For her Slovenian-German online
dictionary, Birte Lnneker was twice awarded the EURALEX Laurence Urdang
Award. From 2002 to 2004, she received various research grants for
Slovenia, where she was working in the Corpus Laboratory of the Institute
of Slovenian Language.<p>
Since 2004, Birte Lnneker carries out research on Story Generator
Algorithms within the Narratology Research Group Hamburg. She is also a
board member of the German Cognitive Linguistics Association.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Jun 05
  </td><td align=left valign=top>
    Gully Burns
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_17');">
    The neuroscience laboratory as a knowledge factory: challenges, approaches and tools
    </a><br>
  <span id=abs05_Jun_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
As a discipline of biology, the field of neuroscience suffers greatly from
information overload, non-standardization and complexity. In the absence
of a mathematical theoretical structure for the subject, scientists use
their own ad-hoc methods of collating and synthesizing information from
both the primary literature and their own data. In order to eventually
formalize and accelerate the development of theoretical approaches in the
subject, we are combining an Electronic Laboratory Notebook (ELN) with
asset management of the primary research literature to construct a
knowledge engineering framework based around the organizational unit of a
neuroscience laboratory. This project, called NeuroScholar
(http://www.neuroscholar.org/) is open-source, and is being tested and
used in the laboratories of Prof. Larry Swanson and Prof. Alan Watts at
USC. In each laboratory, the system will operate on top of a laboratory
corpus of knowledge resources (data files, full-text pdf files , etc.)
that summarizes the relevant knowledge for that laboratory. Not only will
this collection provide a valuable resource for the members of the
laboratory, it provides a platform for natural language processing and
knowledge engineering to answer formally-defined research questions. The
Society for Neurosciences annual meeting attracts over 30,000 attendees,
who collectively form potential user-base of this software.<p>
I will talk about the ideas underlying the project, the current
implementation of NeuroScholar, developments from collaboration with the
natural language group at ISI and possible collaborations for the future.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    13 Jun 05
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_13');">
    Search, Learning and Features (my thesis proposal proposal)
    </a><br>
  <span id=abs05_Jun_13 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm (MONDAY!!!)<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>
I'm going to talk about what I've been working on recently.  My thesis
proposal is something having to do with the interaction of search,
learning and features in supervised natural language problems.  I will be
focusing on the task of coreference, since it is a well-studied problem,
yet nevertheless not really solved and quite difficult.  It is also a 
great pedagogical example for why we should care about something *other* 
than standard Markov random fields for structured prediction, since, for 
the coreference problem (and pretty much every other "real" natural 
language problem) inference in such models is intractable.<p>
The contents of this talk will be roughly 40% from a paper I have at ICML
this year on efficient, accurate supervised learning techniques for
structured prediction (and why I feel inclined to make the very
controversial statement that supervised learning for NLP problems is
solved); it will be roughly 40% about an application of this technique to
the coreference resolution problem and an exploration of the feature space
for solving this problem (submitted to HLT); and it will be roughly 20%
about looking forward to what I want to accomplish in the remainder of my
thesis, not covered by the first 80%.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Jun 05
  </td><td align=left valign=top>
    Liang Huang (Penn)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_10');">
    Better k-best Parsing, Hypergraphs and Dynamic Programming
    </a><br>
  <span id=abs05_Jun_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We discuss the relevance of k-best parsing to recent applications in
natural language parsing, and develop algorithms that substantially
improve on previously-used algorithms with respect to efficiency,
scalability, and accuracy. We demonstrate these algorithms in experiments
on Bikel's implementation of Collins' lexicalized PCFG model, and on a
synchronous CFG based decoder for statistical machine translation. We show
in particular how the improved output of our algorithms has the potential
to improve results from parse reranking systems and other applications.<p>
In this talk, I will demonstrate the convergence of several popular
parsing formalisms (weighted deduction, shared forest, semiring) under the
powerful hypergraph formalism. If time permits, I will also show how
generic Dynamic Programming can be formalised as hypergraph searching.<p>
Joint work with David Chiang (University of Maryland)<p>
<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    08 Jun 05
  </td><td align=left valign=top>
    Hao Zhang (Rochester)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jun_08');">
    Lexicalization and A* Searching for Inversion Transduction Grammar
    </a><br>
  <span id=abs05_Jun_08 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4th floor<br>
<b>Abstract:</b> <br>
The Inversion Transduction Grammar (ITG) of \cite{DekaiCL} generates a
synchronous parse tree for a given pair of sentences in two languages. By
allowing inversion of the order of children at any level of the
synchronous parse tree, ITG can do recursive, systematic word reordering.
We made a version of ITG where the nonterminals are lexicalized by word
pairs and the inversions are dependent on the so-lexicalized nonterminals.  
We found out that after lexicalization, the Alignment Error Rate (AER)
against gold standard is reduced for short sentences. ITG parsing
complexity is high polynomial. We proposed a pruning techique that
utilizes IBM Model 1 to estimate the inside and outside probability of a
bitext cell. Taking a step further, we applied the A* parsing having been
used for monolingual parsing to ITG.  I will talk about the heuristic
estimates we used for A* parsing for Viterbi alignment selection and
decoding.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 May 05
  </td><td align=left valign=top>
    Radu Soricut
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_May_27');">
    Towards Developing Generation Algorithms for Text-to-Text
    </a><br>
  <span id=abs05_May_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>
We describe a new sentence realization framework for text-to-text
applications. This framework uses IDL-expressions as a representation
formalism, and a generation mechanism based on algorithms for intersecting
IDL-expressions with probabilistic language models. We present both
theoretical and empirical results concerning the correctness and
efficiency of these algorithms.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    13 May 05
  </td><td align=left valign=top>
    Ed Stabler (UCLA)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_May_13');">
    Natural Logic
    </a><br>
  <span id=abs05_May_13 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will describe some recent work on "natural logics", logics for languages
that are more similar to human languages than traditional first order
predicate logic, giving particular attention to questions about what the
syntax encodes about semantic relations among sentences. On everyone's
view, some but not all entailments are syntactically encoded (in a sense
that I will define precisely), but, beyond this starting point,
controversy starts almost immediately. Considering some particular
examples, I will sketch methods for addressing some of the basic
questions.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Apr 05
  </td><td align=left valign=top>
    Deepak Ravichandran
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Apr_22');">
    Working with Large Corpus, High speed clustering and its applications
    </a><br>
  <span id=abs05_Apr_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I am going to be talking about stuff that I have been working over the
past 6-9 months. This includes randomized algorithms and its application
to 2 NLP problems: noun clustering and noun-pair clustering. I will also
be commenting on my experience of working with very very large amounts of
real Natural Language text (This includes processing and working with data
available from the web. This corpus is not the standard newspaper text
that we are so used to in the NLP community.) This talk will also cover a
large part of my thesis work.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    08 Apr 05
  </td><td align=left valign=top>
    Jamie Callan (CMU)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Apr_08');">
    Search Engines for HLT Applications
    </a><br>
  <span id=abs05_Apr_08 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Mar 05
  </td><td align=left valign=top>
    Dagen Wang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Mar_25');">
    Metalinguistic feature study for spontaneous speech in human computer interaction
    </a><br>
  <span id=abs05_Mar_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large (THIS HAS CHANGED!!!)<br>
<b>Abstract:</b> <br>
Speech is a crucial component in human computer interaction. While
tremendous progress has been made in automatic speech recognition, speech
transcription -- which is the output of automatic speech recognition -- is
far from providing all the information that one could retrieve from
speech. For example, prominence, pause, rhythm, and rate of speech all
carry important information in speech and are crucial in speech
perception. Inclusion of such information can facilitate better machine
recognition and understanding of speech.<p>
In this talk, we will introduce the research effort and result in speech
rate, prominence, disfluency and utterance boundary detection. We will
also show some interesting applications utilizing these features in
natural language understanding and dialog management.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Mar 05
  </td><td align=left valign=top>
    Ed Hovy
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Mar_18');">
    Methodologies of ontology content construction
    </a><br>
  <span id=abs05_Mar_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk is the second in three tutorial lectures on ontologies.  It
first shows some details of various Upper Ontologies-ResearchCYC, SUMO,
DOLCE, and the Penman Upper Model.  It then discusses the problem of
creating content for the 'Middle Model' zone of ontologies, and outlines a
methodology for moving from words to word senses to concepts.  It
concludes by describing ISI's Omega ontology and showing how Omega has
been used in annotation projects to support semantic labeling of texts.<p>
Please bring a pen or pencil and some paper; there is a small exercise!<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Feb 05
  </td><td align=left valign=top>
    Inderjeet Mani (Georgetown)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Feb_18');">
    TBA
    </a><br>
  <span id=abs05_Feb_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    14 Feb 05
  </td><td align=left valign=top>
    Tim Chklovski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Feb_14');">
    Collecting Broad-Coverage Knowledge Bases from Volunteers
    </a><br>
  <span id=abs05_Feb_14 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(Note that this is a MONDAY!)<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    11 Feb 05
  </td><td align=left valign=top>
    Hae-Chang Rim
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Feb_11');">
    Unsupervised Word Sense Disambiguation Using Wordnet Relatives
    </a><br>
  <span id=abs05_Feb_11 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    28 Jan 05
  </td><td align=left valign=top>
    Yutaka Sasaki (ATR)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs05_Jan_28');">
    Research Activities in Speech Translation at ATR/QA as Question-Biased Term Extraction
    </a><br>
  <span id=abs05_Jan_28 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk has two parts. In the first part, I will introduce research
activities in Speech-to-Speech Translation at ATR, including on-going
research on statistical machine translation. In the second part, I will
present a new approach to QA named Question-Biased Term Extraction (QBTE).
The QBTE directly extracts answers as terms biased by the question. To
confirm the feasibility of our QBTE approach, we conducted experiments on
the CRL QA Data based on 10-fold cross validation, using Maximum Entropy
Models as an ML technique. Experimental results showed that the trained
system achieved approximately 0.35 in MRR and 50% in TOP5 accuracy. This
part is an English version of my presentation given in IPSJ SIGNL-163 in
2004 in Japanese. If time allows, I would like to introduce the NTCIR-5
(2004/2005) Cross-Lingual QA task (CLQA) that I am going to organize.<p>
About the speaker:<p>
Yutaka Sasaki received his Ph.D. in Engineering from the University of
Tsukuba, Japan in 2000 for his work on generating Information Extraction
rules with hierarchically sored Inductive Logic Programming. He joined NTT
Laboratories in 1988. Since then, he was involved in research in
rule-based CAI, inductive logic programming, Information Extraction, and
Question Answering. From 1995 to 1996, he spent one year at Simon Fraser
University, Canada as a visiting researcher. From 1999, he led a subgroup
to develop the first practical Japanese Question Answering System SAIQA.
Then, he applied SVMs to automatically construct the QA system SAIQA-II
from QA and NE data. In June 2004, he moved to ATR Spoken Language
Translation Research Laboratories. Currently, he is the head of Department
of Natural Language Processing. He is also an organizer of the NTCIR 5
Cross-Lingual Question Answering Task.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Dec 04
  </td><td align=left valign=top>
    Nicola Ueffing
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Dec_17');">
    Word-Level Confidence Measures for SMT
    </a><br>
  <span id=abs04_Dec_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk will address the problem of assessing the correctness of MT
output on the word level. I will give an overview on word confidence
measures for SMT.  Different variants of word posterior probabilities that
can be directly used as confidence measure will be presented. Their
connection with the Bayes decision rule and the underlying error measure
will be shown. Experimental comparison of different word confidence
measures will be presented on a translation task consisting of technical
manuals.<p>
Additionally, I will show how word confidence measures can be applied in
an interactive SMT system. This system predicts translations, taking parts
of the sentence into account that have already been accepted or typed by
the user. Through the use of confidence measures, the performance of the
prediction engine can be improved.<p>

About the Speaker:<p>
Nicola Ueffing is a graduate research assistant at the group for "Human
Language Technology and Pattern Recognition" (Lehrstuhl fuer Informatik
VI) at RWTH Aachen University. She received her diploma in mathematics
from RWTH Aachen University in 2000. Her research topic is statistical
machine translation, focusing on confidence measures for SMT. In 2003, she
was a member of the team working on "Confidence Estimation for SMT" at the
CLSP workshop at JHU.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Dec 04
  </td><td align=left valign=top>
    Nick Mote
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Dec_10');">
    Developing a Language Model for Second Language Learner Speech
    </a><br>
  <span id=abs04_Dec_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
ISI's Tactical Language Project is a system designed to teach Americans
how to speak Arabic through a video game environment. We've taken a FPS
engine (Unreal 2003) and re-did the graphics so it looks like you're in a
typical Lebanese village. We took away the guns, added speech recognition,
and set the player in the middle of it all. The theory is that if you
learn well in a classroom, you'll perform well in a classroom, but if you
learn well in a pseudo-naturalistic environment, you'll perform better in
real life.<p>
In a pedagogical context, speech recognition is a hard thing we're trying
to recover signal from noisy language-learner speech--with all of its
mispronunciations, disfluencies, and grammatical errors . Language
understanding is hopeless unless you have a good approximation of what
kinds of mistakes learners make, and you can build a system to anticipate
them.<p>
Suppose an English language learner says "Water". Is he asking you for
water? Is he telling you there's a puddle in front of you? Is he saying
his name is "Walter", but with horrible pronunciation? There's a lot of
ambiguity involved. In order to disambiguate, we need to look at the
speech signal itself, the utterance's context, the learner's past language
performance, and details about the learner's mother language as it relates
to English, etc., etc... Only then can we hope to guess what the learner
is actually trying to say.<p>
And then, of course, once we've made a good guess at the learner's speech
intentions, what do we do about it? How do we correct him? How do we
balance the consideration of inherent qualities of learner motivation,
language errors, learning objectives, and possibly low-confidence speech
recognition, as we generate good pedagogical feedback?<p>
This is NLP (primarily statistical) with a bit of pedagogy theory and
linguistic (SLA and phonology) theory sprinkled in.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    19 Nov 04
  </td><td align=left valign=top>
    Chin-Yew Lin
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Nov_19');">
    After TIDES, What's Left? - Finding Basic Elements
    </a><br>
  <span id=abs04_Nov_19 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
As DARPA's TIDES (Translingual Information Detection, Extraction, and
Summarization) program coming to an end, I will give a summary of what we
have learned from TIDES in summarization and a brief overview of our
current effort in developing automatic evaluation methods that go beyond
surface n-gram matching. Topics to be covered:<p>
(1) Summary of DUCs 2001 - 2004
(2) Automatic Evaluations in Summarization and MT
(3) Basic Elements - New Efforts in Summarization at ISI

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Nov 04
  </td><td align=left valign=top>
    Thiago Pardo
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Nov_15');">
    Unsupervised learning of verb argument structures
    </a><br>
  <span id=abs04_Nov_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm (note the strange date!)<br>
<b>Location:</b> 8th floor multipurpose room (#849) -- NOT the conference room<br>
<b>Abstract:</b> <br>
In this talk, I'll present the investigation I'm carrying out in ISI
lately under Daniel Marcu's supervision.  Following the noisy-channel
framework, we propose a statistical model for learning the argument
structures of verbs automatically.  We show that we are able to learn both
lexicalized and generalized structures and achieve good results, relying
only on basic NLP tools like a POS tagger and named-entity recognizer. We
also present a comparison of the structures we learn with the predicted
ones in PropBank.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Nov 04
  </td><td align=left valign=top>
    Dragomir Radev
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Nov_12');">
    Words, links, and patterns: novel representations for Web-scale text mining
    </a><br>
  <span id=abs04_Nov_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Textual data is everywhere, in email and scientific papers, in online
newspapers and e-commerce sites. The Web contains more than 200 terabytes
of text not even counting the contents of dynamic textual databases. This
enormous source of knowledge is seriously underexploited. Textual
documents on the Web are very hard to model computationally: they are
mostly unstructured, time-dependent, collectively authored, multilingual,
and of uneven importance.  Traditional grammar-based techniques don't
scale up to address such problems. Novel representations and analytical
tools are needed.<p>
I will discuss several current projects at Michigan related to text mining
from a variety of genres. Depending on the amount of time, I will talk
about (a) lexical centrality for multidocument summarization, (b)
syntax-based sentence alignment, (c) graph-based classification,(d)
lexical models of Web growth, and (e) mining protein interactions from
scientific papers. As it turns out, the right representations, when
complemented with traditional NLP and IR techniques, turn many of these
into instances of better studied problems in areas such as social
networks, statistical mechanics, sequence analysis, and computational
phylogenetics.<p>
<p>
About the Speaker:<p>
Dragomir R. Radev is Assistant Professor of Information, Electrical
Engineering and Computer Science, and Linguistics at the University of
Michigan, Ann Arbor.  He leads the CLAIR (Computational Lingusitics
And Information Retrieval) group which currently includes 12
undergraduate and graduate students.  Dragomir holds a Ph.D. in
Computer Science from Columbia University.  Before joining Michigan,
he was a Research Staff Member at IBM's TJ Watson Research Center in
Hawthorne, NY.  He is the author of more than 45 papers on information
retrieval, text summarization, graph models of the Web, question
answering, machine translation, text generation, and information
extraction.  Dr. Radev's current research on probabilistic and
link-based methods for exploiting very large textual repositories,
representing and acquiring knowledge of genome regulation, and
semantic entity and relation extraction from Web-scale text document
collections is supported by NSF and NIH.  Dragomir serves on the
HLT-NAACL advisory committee, was recently reelected as treasurer of
NAACL, is a member of the editorial boards of JAIR and Information
Retrieval, and is a four-time finalist at the ACM international
programming finals (as contestant in 1993 and as coach in
1995-1997). Dragomir received a graduate teaching award at Columbia
and recently, the U. of Michigan award for Outstanding Research
Mentorship (UROP).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Nov 04
  </td><td align=left valign=top>
    Mary Wood (Manchester)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Nov_05');">
    A Human-Computer Collaborative Approach to Computer Aided Assessment
    </a><br>
  <span id=abs04_Nov_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The ABC (Assess by Computer) system has been developed and used in the
School of Computer Science at the University of Manchester for formative
and (principally) summative assessment at undergraduate and postgraduate
level. We believe that fully automatic marking of constructed answers -
especially free text answers - is not a sensible aim. Instead - drawing on
parallels in the history of machine translation - we take a
"human-computer collaborative" approach, in which the system does what it
can to support the efficiency and consistency of the human marker, who
keeps the final judgement.<p>
Our current work focuses on what are generally referred to as "short text
answers" as contrasted to "essays". However we prefer to contrast
"factual" with "discursive" answers, and speculate that the former may be
amenable to simple statistical techniques, while the latter require more
sophisticated natural language analysis. I will show some examples of real
exam data and the techniques we are using and developing to handle them.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Oct 04
  </td><td align=left valign=top>
    Jerry Hobbs
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Oct_22');">
    Like Now:  Two Explorations in Deep Lexical Semantics
    </a><br>
  <span id=abs04_Oct_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
As part of an effort to encode the commonsense knowledge we need in
natural language understanding, I have been looking at several very common
words and their uses in diverse corpora, and asking what we have to know
to understand this word in this context.  In this talk, I will describe
the investigations of the uses of two words -- the adverb "now" and the
preposition "like".<p>
One might think that "now" simply expresses a temporal property of an
event.  But in fact in almost every instance, it is used to point up a
contrast -- "This is true now.  Something else was true then."  It is thus
more of a relation than a property.  I will describe several categories of
such relations.  Another question of interest about "now" is "How long a
period is the word "now" describing in its various uses?": "I'm typing an
abstract now" vs. "We travel by automobile now."  I suggest some
categories of knowledge that need to be encoded to answer this question.<p>
When we successfully understand "A is like B", we have figured out some
property that A and B have in common.  How can we find that property
computationally?  In the data I looked at, in 80% of the instances, the
property is explicit in the nearby text, and I will talk about how we can
identify it.  For the remainder I examine the knowledge we would need in
order to infer the common property.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Sep 04
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Sep_24');">
    Domain Adaptation in Maximum Extropy Models
    </a><br>
  <span id=abs04_Sep_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will present some preliminary results on the problem of domain 
adaptation in maximum entropy models, specifically in the case when there 
is a large amount of "out of domain" data, and only a very small amount of 
"in domain" data.  The model and algorithms I present are based on the 
technique of conditional Expectation Maximization (CEM) and allow for 
relatively fast optimization of these models.  Preliminary results on some 
tasks are quite promising.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Sep 04
  </td><td align=left valign=top>
    Various
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Sep_17');">
    About Syntax Fest 2004 (Part II)
    </a><br>
  <span id=abs04_Sep_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This summer we held a three-month workshop on syntax-driven machine 
translation, in which we learned syntactic transformations automatically
from Chinese/English translated corpora and applied them to translate new
text.  We'll give a progress report!<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Sep 04
  </td><td align=left valign=top>
    Various
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Sep_10');">
    About Syntax Fest 2004 (Part I)
    </a><br>
  <span id=abs04_Sep_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This summer we held a three-month workshop on syntax-driven machine
translation, in which we learned syntactic transformations automatically
from Chinese/English translated corpora and applied them to translate new
text.  We'll give a progress report!<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Aug 04
  </td><td align=left valign=top>
    Patrick Pantel & Tim Chklovski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Aug_16');">
    VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations
    </a><br>
  <span id=abs04_Aug_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:00 pm - 3:30 pm (note the strange time)<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Broad-coverage repositories of semantic relations between verbs could
benefit many NLP tasks. We present a semi-automatic method for extracting
fine-grained semantic relations between verbs. We detect similarity,
strength, antonymy, enablement, and temporal happens-before relations
between pairs of strongly associated verbs using lexico-syntactic patterns
over the Web. On a set of 29,165 strongly associated verb pairs, our
extraction algorithm yielded 65.5% accuracy. We provide the resource,
called VerbOcean, for download at http://semantics.isi.edu/ocean/. We will
also discuss current work on disambiguating the verbs in the network as
well as refining the semantic relations using path analysis.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    13 Aug 04
  </td><td align=left valign=top>
    Deepak Ravichandran
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Aug_13');">
    Randomized algorithms and its application to NLP
    </a><br>
  <span id=abs04_Aug_13 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The last decade has seen a plethora of papers in NLP devoted to Machine
Learning algorithms. However, most of these papers have devoted their
effort exclusively to improving the system performance on the accuracy
axis. Most of the sophisticated NLP algorithms are extremely slow and do
not scale up easily when applied to large amounts of data.<p>
I will talk about the importance of randomized algorithms and their
potential in speeding up some NLP algorithms. This talk will be a survey
of some recent advances in Theoretical Computer Science/Math seen with an
NLP point-of-view. I am not going to present any results. But I am hoping
that this talk will clarify my thinking process, get feedback from people
and help me colloborate with others.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 Aug 04
  </td><td align=left valign=top>
    Justin Busch, Hai Huang, Jens Stephan & Chen-kang Yang
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Aug_09');">
    CL Student Presentations
    </a><br>
  <span id=abs04_Aug_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Justin Busch:
Weight and Semantic Class Issues in Japanese Noun Phrase Ordering<p>
Many current designs for automatic parsers learn probabilities for the
relative frequencies of parts-of-speech and syntactic rules, and this has
proven to be generally reliable. In spite of the ubiquity of probabilistic
techniques for parsing, however, little attention has been given to the
linguistic significance of the probabilistic data and what it might say
about human performance.<p>
Hawkins proposes a general theory of grammaticalization based on the
minimization of syntactic domains. Given that a sentence of any language
will contain at least one noun phrase, one verb, and possibly additional
noun phrases and prepositional phrases, "minimize domains" suggests that
these phrases will order themselves according to whichever pattern
requires the least effort to recognize the higher syntactic structure of
the sentence. These effects are directly measurable through corpus
statistics, and can be interpreted as potential heuristics for
probabilistic parsers.  In this study, we examine Japanese data from the
Kyoto Treebank and test Hawkins' predictions for noun phrase ordering by
noun phrase weight as well as by generic semantic types. The discussion
will focus primarily on how accurately Hawkins' predictions are reflected
in the corpus statistics, and will conclude with observations about how
they might be applied to the decision mechanisms of probabilistic parsers.<p>
--------------------------------------------------------------------------<p>
Hai Huang:
TBA<p>
--------------------------------------------------------------------------<p>
Jens Stephan:
Evaluation and Visualization of a Dialogue System<p>
Evaluations have become a necessary standard to almost any type of
research. However, there are many areas where there is no common agreement
on how to evaluate, which is the case for complex problem of evaluating
dialogue systems. The evaluation of the multi party multi modal dialogue
system MRE(1) provides a good example of what questions are important for
such an evaluation, how to actually do the evaluation and finally how to
how make special problems of the system visible to use the evaluation
results to improve the systems performance.<p>
After a brief introduction of the MRE domain and architecture, I will
break the task town to a set of general evaluation questions. From there I
will explain what kinds of metrics and visualizations are suited to answer
those questions and what kind of data is needed, as well as how that data
was obtained. Along the road, examples of actual system problems and
performances will be presented. The topics of data formatting and
visualization will receive some special attention by introducing the MRE
Evaluation Toolkit as well as the corpus it operates on.<p>
--------------------------------------------------------------------------<p>
Chen-kang Yang:
Using the Omega Ontology to Determine Selectional Restrictions for Word Sense Disambiguation<p>
Word sense disambiguation is fundamental for language processing. Though
purely statistical methods are effective for this task, they neglect the
syntactic and semantic aspects. In this study, we adopt a hybrid approach
by applying an unsupervised machine learning method to learn verbs
selectional restrictions on their subjects/objects. The system then uses
these learned selectional restrictions for word sense disambiguation of
the subjects/objects. Instead of words, the training data contains
ontological taxonomy hierarchies that are retrieved from the Omega
ontology. Unlike other similar systems, we are able to automatically find
the best match among classes from different levels of the ontology. This
provides us more flexibility and is closer to human instinct. Our system
performs better than other similar systems, though it still needs
cooperating methods for better results.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    06 Aug 04
  </td><td align=left valign=top>
    Hae-Chang Rim
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Aug_06');">
    Information Retrieval using Word Senses: Root Sense Tagging Approach
    </a><br>
  <span id=abs04_Aug_06 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Information retrieval using word senses is emerging as a good research
challenge on semantic information retrieval. In this presentation, I am
going to propose a new method using word senses in information retrieval:
root sense tagging method. This method assigns coarse-grained word senses
defined in WordNet to query terms and document terms by unsupervised way
using co-occurrence information constructed automatically. The sense
tagger is crude, but performs consistent disambiguation by considering
only the single most informative word as evidence to disambiguate the
target word. We also allow multiple-sense assignment to alleviate the
problem caused by incorrect disambiguation.<p>
Experimental results on a large-scale TREC collection show that the
proposed approach to improve retrieval effectiveness is successful, while
most of the previous work failed to improve performances even on small
text collection. The proposed method also shows promising results when is
combined with pseudo relevance feedback and state-of-the-art retrieval
function such as BM25.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Jul 04
  </td><td align=left valign=top>
    Hal Daume III and Radu Soricut
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jul_16');">
    Practice Talks for ACL (+workshops)
    </a><br>
  <span id=abs04_Jul_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 Jul 04
  </td><td align=left valign=top>
    Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jul_09');">
    Survey of Trees and Grammars
    </a><br>
  <span id=abs04_Jul_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I'll give a survey of trees and grammars, at least the parts that seem
most relevant to ongoing work at ISI.  This will be a theory talk.  I'll
start with context-free grammars, which were developed in the 1950s, and
cover other tree-generating systems.  I'll also talk about
tree-transforming systems.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 Jul 04
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jul_02');">
    A Phrase-Based HMM Approach to Document/Abstract Alignment
    </a><br>
  <span id=abs04_Jul_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 1:30 pm - 3:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will present work that extends the standard hidden Markov model to a
version that can emit multiple symbols in a single time step.  Using this
model, we are able to automatically create phrase-to-phrase mappings in an
alignment process.  I've applied this model to the task of creating
alignments between documents and their human-written abstracts, yielding
an overall alignment F-score of 0.548, a significant improvement on the
best results to date of 0.363.  These results are published in an EMNLP
paper this year, but the talk will be an extended version of the talk I
will give there (namely, I will discuss the mechanics of the extended HMM
in more detail in this seminar).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Jun 04
  </td><td align=left valign=top>
    Dan Gildea
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jun_25');">
    Syntactic Supervision and Tree-Based Alignment
    </a><br>
  <span id=abs04_Jun_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Tree-based probability models of translation have been proposed to take
advantage of parse trees on one, both, or neither sides of a parallel
corpus.  I will present comparative results for these three approaches for
the task of word alignment on Chinese-English and French-English data, as
well as some analysis of what is going on behind the numbers.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    21 Jun 04
  </td><td align=left valign=top>
    Emil Ettelaie
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jun_21');">
    Speech-to-Speech Translation: A Phrase Classification Approach
    </a><br>
  <span id=abs04_Jun_21 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk will be about automatic speech-to-speech translation.  In our
system, a doctor speaks one language, the patient speaks another language,
and the machine translates their utterances from one language to the
other.  The talk will be followed by a demo of our system.<p>
One approach we have been successful with is phrase classification, i.e.,
classifying a noisy speech-recognized utterance into one of many meaning
categories.  Phrase classification is computationally cheap and can
provide high quality translations for in domain utterances almost
instantaneously. Speed is important for speech translation, where
processing delay is a great concern.<p>
In this talk, different aspects of building a classification-based speech
translator are discussed. Following an overview of automatic
speech-to-speech translation and its challenges, a comparison of different
classification methods is presented and data collection techniques for
that application are introduced.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Jun 04
  </td><td align=left valign=top>
    Marcello Federico
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jun_17');">
    Statistical Machine Translation at ITC-irst
    </a><br>
  <span id=abs04_Jun_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4th Floor<br>
<b>Abstract:</b> <br>
My presentation will overview recent activities on Chinese-English SMT
carried out at ITC-irst (Trento, Italy).  After an overview of the
complete architecture of our system, I will focus on progress made in
Chinese word-segmentation, phrase-based modeling and decoding, log-linear
modeling and minimum error training, and language model adaptation.
Experimental results will be provided in terms of Bleu and Nist scores on
two translation tasks:  basic traveling expressions and news reports,
respectively adopted by the C-STAR consortium and for the 2002 and 2003
NIST MT evaluation campaigns.<p>
Bio:<p>
Marcello Federico has been a permanent researcher at ITC-irst since 1991.  
During 1998-2003, he led the "Multilingual natural speech technologies"
(MUNST)  research line at ITC-irst.  Since 2004, he is head of the
"Cross-language information processing" (Hermes) research line. His
interests include automatic speech recognition, statistical language
modeling, information retrieval, and machine translation.<p>


</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 May 04
  </td><td align=left valign=top>
    Philipp Koehn
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_May_24');">
    Challenges in Statistical Machine Translation
    </a><br>
  <span id=abs04_May_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 4:00 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In the last years a standard model in statistical machine
translation has emerged, which is based on the translation
of sequences of words (so-called "phrases") at a time.
I will describe this model, how to train and decode with it,
but the focus of this talk will be how to address the
challenges to advance and move beyond the model: my thesis
work on noun phrase translation, making use of syntax, and
better modeling, such as discriminative training.<p>
Bio: Philipp Koehn is the author of papers on natural language
processing, machine translation, and machine learning. He
received his PhD from the University of Southern California
in 2003 (advisor: Kevin Knight), and is currently employed as
a postdoc at the Massachusetts Institute of Technology, working
with Michael Collins. He has worked at AT&T Laboratories on
text-to-speech systems, and at WhizBang! Labs on text
categorization.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    21 May 04
  </td><td align=left valign=top>
    Tom Murray and Rahul Bhagat
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_May_21');">
    Statistical Learning for Dialogue System <b>and</b> A Community of Words
    </a><br>
  <span id=abs04_May_21 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
<b>Natural Language Understanding: A fast and accurate Statistical Learning Approach for Dialogue Systems</b><p>
Natural Language Understanding (NLU) is an essential module of a good
dialogue system. To achieve satisfactory performance levels, real time
dialogue systems need the NLU module to be both fast and accurate. Finite
State Model (FSM) based systems are fast and accurate but lack robustness
and flexibility. The Statistical Learning Model (SLM) based systems are
robust and flexible but lack accuracy and are at most times slow.<p>
In this talk, I am going to talk about an SLM based NLU approach for
dialogue utterances that is both accurate and fast. The system has high
accuracy and produces frames in real time.<p>
<b>A Community of Words: Understanding Social Relationships from E-mail</b><p>
A corpus of e-mail messages presents a number of challenges for NLP
techniques, with its nearly unconstrained structure and vocabulary,
mistyped words and ungrammatical sentences, and extensive contextual
information that is never explicitly stated. Yet, the intrinsically social
nature of such communication provides an opportunity to study not just a
bag of words, but also the relationships, competencies, and activities
behind them.<p>
This talk presents work with Eduard Hovy as part of the MKIDS project.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 Apr 04
  </td><td align=left valign=top>
    Liang Zhou
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_30');">
    Automating the Building of Summarization Systems
    </a><br>
  <span id=abs04_Apr_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Summarization requires one to identify the internal structure of
information and to bring that to the surface both operationally and
organizationally.<p>
How does one put this theory to practice and build real summarization
systems? How do the systems built based on this idea perform?<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    28 Apr 04
  </td><td align=left valign=top>
    Dragos Muntanu, Radu Soricut and Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_28');">
    Practice Talks for HLT/NAACL
    </a><br>
  <span id=abs04_Apr_28 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 Apr 04
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_23');">
    A Tree-Position Kernel for Document Compression
    </a><br>
  <span id=abs04_Apr_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 10 Large<br>
<b>Abstract:</b> <br>
I'll describe our entry into the DUC 2004 automatic document summarization
competition.  We competed only in the single document, headline generation
task.  Our system is based on a novel kernel dubbed the tree position
kernel, combined with two other well-known kernels.  Our system performs
well on white-box evaluations, but does very poorly in the overall DUC
evaluation.  C'est la vie.

<p><b>Slides:</b> 
<a href="slides/04-tree-position-kernel.ps.bz2">04-tree-position-kernel.ps.bz2</a> 
<a href="slides/04-tree-position-kernel.pdf">04-tree-position-kernel.pdf</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Apr 04
  </td><td align=left valign=top>
    Rada Mihalcea (UNT)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_16');">
    Graph-based Ranking Algorithms for Language Processing
    </a><br>
  <span id=abs04_Apr_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Although we live in a predominantly statistical world, there are still
many language processing applications that long for accurate
representations of text meaning. Even applications that found partial
solutions in statistical modeling, including information retrieval,
machine translation, or automatic summarization, are likely to get a
significant boost from deeper text understanding.<p>
In this talk, I will present an innovative method for automatic extraction
of conceptual graphs as a means to represent text meaning. The method
relies on a novel adaptation of graph-based ranking algorithms -
traditionally (and successfully) used in citation analysis, Web page
ranking, and social networks. I will show how such algorithms can be
adapted to semantic networks, resulting in an efficient unsupervised
method for resolving the semantic ambiguity of all words in open text, and
identifying relations between entities in the text. I will also outline a
number of applications that are enabled by this representation, including
keyphrase extraction, domain classification, and extractive summarization.<p>
BIO: Rada Mihalcea is an Assistant Professor of Computer Science at
University of North Texas. Her research interests are in lexical
semantics, minimally supervised natural language learning, and
multilingual natural language processing. She is currently involved in a
number of research projects, including word sense disambiguation, shallow
semantic parsing, (non-traditional) methods for building annotated corpora
with volunteer contributions over the Web, word alignment for language
pairs with scarce resources, and graph-based ranking algorithms for
language processing. Her research is supported by NSF and the state of
Texas.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    13 Apr 04
  </td><td align=left valign=top>
    Jill Burstein (ETS)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_13');">
    Automated Essay Evaluation: From NLP research through deployment as a business
    </a><br>
  <span id=abs04_Apr_13 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 4 Large<br>
<b>Abstract:</b> <br>
Automated essay scoring was initially motivated by its potential cost
savings for large-scale writing assessments.  However, as automated essay
scoring became more widely available and accepted, teachers and assessment
experts realized that the potential of the technology could go way beyond
just essay scoring.  Over the past five years or so, there has been rapid
development, and commercial deployment of automated essay evaluation for
both large-scale assessment and classroom instruction.  A number of
factors contribute to an essay score, including varying sentence
structure, grammatical correctness, appropriate word choice, errors in
spelling and punctuation, use of transitional words/phrases, and
organization and development. Instructional software capabilities exist
that provide essay scores and evaluations of student essay writing in all
of these domains.  The foundation of automated essay evaluation software
is rooted in NLP research.  This talk will walk through the development of
CriterionSM, e-rater, and Critique writing analysis tools, automated essay
evaluation software developed at Educational Testing Service - from NLP
research through deployment as a business.<p>
(Preview of an HLT/NAACL-2004 Invited Speaker Presentation)<p>
Jill Burstein
Educational Testing Service
Princeton, NJ<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 Apr 04
  </td><td align=left valign=top>
    Eduard Hovy
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_09');">
    Three (and a half?) Trends: The Future of NLP
    </a><br>
  <span id=abs04_Apr_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
An interesting (disturbing?) new trend is beginning to manifest itself in
NLP, one that is focused on performance and hence very attractive in the
context of inter-system competitive evaluations such as TREC and DUC, but
one that does not provide much insight about language or NLP methods to
the researcher interested in these topics.  This addition of a new
paradigm to NLP has implications for all of us.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 Apr 04
  </td><td align=left valign=top>
    Stephan Vogel
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Apr_02');">
    The CMU Statistical Machine Translation System
    </a><br>
  <span id=abs04_Apr_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The presentation will give an overview of the SMT activities at the
Language Technologies Institute, Carnegie Mellon University, in large
vocabulary text translation tasks, esp. the Chinese-English and
Arabic-English, as well as in limited domain speech-to-speech translation
tasks.  The CMU SMT system is, like most modern statistical MT systems,
based on phrase translation.  Several approaches have been developed to
extract the phrase pairs from parallel corpora and current research
investigates different scoring approaches for these translation pairs.
Details of the decoder, esp. on hypothesis recombination, pruning, and
efficient n-best list generation will be given.  Recently, the SMT system
has been extended to use partial translations generated from example based
and grammar based translation system, thereby performing multi-engine
machine translation.<p>
Bio:<p>
Stephan Vogel is a researcher at the Language Technologies Institute,
Carnegie Mellon University, where he heads the statistical machine
translation team.  He received a Diploma in Physics from Philips
University Marburg, Germany, and a Masters of Philosophy from the
University of Cambridge, England.  After working for a number of years on
the history of science, he turned to computer science, especially natural
language processing.  Before coming to CMU, he worked for several years at
the Technical Univerity of Aachen on statistical machine translation, and
also in the Interactive Systems Lab at the University of Karlsruhe.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    26 Mar 04
  </td><td align=left valign=top>
    Shlomo Argamon
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Mar_26');">
    On Writing, Our Selves: Explorations in Stylistic Text Categorization
    </a><br>
  <span id=abs04_Mar_26 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 1:30 pm - 3:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
This talk will survey results of several recent projects we have been
undertaking in automated text categorization based upon the style,
rather than the topic, of the documents.  I will describe a general
text-categorization framework using machine learning along with general
principles for choosing stylistically relevant sets of features for
learning effective classification models.  Applications of these methods
include determining author gender and text genre in published books and
articles, authorship attribution of email messages, and analysis of
language use in different scientific fields.  In many cases, the models
that are learned also give some insight into the respective styles being
distinguished, which I will also discuss.<p>
Shlomo Argamon is an associate professor at the Illinois Institute of
Technology Chicago.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Mar 04
  </td><td align=left valign=top>
    Jon Patrick (U. of Sydney)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Mar_25');">
    ScamSeek: Capturing Financial Scams at the Coalface by Language Technology
    </a><br>
  <span id=abs04_Mar_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The Scamseek project aims to build a surveillance tool for identifying
financial scams on the Internet by performing document classification of
Internet pages. There are three principle types of documents of concern:
those that give financial advice by unregistered advisors, unlawful
investment schemes, and share ramping.<p>
The first phase of the project has been completed and a working system,
known as ScamAlert installed at the Australian Securities and Investment
Commission (ASIC). The independent audit of the performance of the system
proved satisfactory with a result for precision of .75, recall .43, and
F=. 54, along with identification of 4 scams misclassified by the client.
Significant improvement in recall is foreshadowed in the 2nd phase of the
project.  The results are satisfying in the context of the structure of
the data where the density of scam documents is about 1.8% of the total
corpus.<p>
The good performance of the operational system is ascribed to the
combination of using a strong linguistic model of language (Systemic
Functional Linguistics) to define the scam documents in parallel with a
rich statistical analysis of the structure of non-scam documents and scam
look-alikes. A large amount of the experimental program has concentrated
on understanding and exploiting the interaction between the linguistically
described aspects of the documents and the statistical properties. Each
type of data has been used to inform and modify the usage of the other.<p>
The operational aspects of the project have proven to be as challenging as
the research objectives. The project has a budget of $2.2M over 15 months.
It has been managed so as to create a balance in resources between the
needs of both the research objectives and the engineering objectives.
Software development has concentrated on three aspects. Firstly, to
produce an environment for the strong directive management of
computational linguistics experiments, secondly, in the aid of the
linguists to create tools to support their manual analysis, and thirdly
the best practice of software engineering principles to ensure a clean
automated rollout of the production system for ASIC.<p>
The contributing partners in the Scamseek project are The Capital Markets
Co-operative Research Centre (CMCRC), ASIC, the University of Sydney and
Macquarie University.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Mar 04
  </td><td align=left valign=top>
    Deepak Ravichandran
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Mar_12');">
    About My Thesis Proposal
    </a><br>
  <span id=abs04_Mar_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
TBA<p>

<p><b>Slides:</b> 
<a href="slides/TP.pdf">TP.pdf</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    20 Feb 04
  </td><td align=left valign=top>
    Hal Daume III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Feb_20');">
    Some Results in Automatic Evaluation for Summarization and MT
    </a><br>
  <span id=abs04_Feb_20 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 4 Large<br>
<b>Abstract:</b> <br>
I will be presenting some recent results of mine regarding the possibility
of automatic evaluation in summarization.  I will discuss both my own 
findings, as well of those of people here and at Columbia, and attempt to 
explain in a principled fashion why there are disparate opinions on the 
plausibility of performing automatic evaluation in this task.  I will
discuss my (perhaps pessimistic) views on the plausibility of doing any
sort of evaluation of summarization, automatic or otherwise.<p>
The results and experimental setups developed in connection with 
summarization will be extended to the machine translation.  I will review 
possible reasons why metrics such a bleu have experienced significantly 
more success in machine translation than in summarization.  I will also 
connect the evaluation criterea developed in the context of summarization 
to machine translation, and discuss the automation of these methods.<p>
In short: I'll talk about why I've been doing so much data elicitaiton 
recently.<p>
This will be a highly informal seminar and participation is highly
encouraged.<p>

<p><b>Slides:</b> 
<a href="slides/sumeval.ps">sumeval.ps</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    06 Feb 04
  </td><td align=left valign=top>
    Mark Hopkins
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Feb_06');">
    What's in a Translation Rule?
    </a><br>
  <span id=abs04_Feb_06 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We propose a theory that gives formal semantics to word-level
alignments defined over parallel corpora. We use our theory to
introduce a linear algorithm that can be used to derive from
word-aligned, parallel corpora the minimal set of syntactically
motivated transformation rules that explain human translation data.<p>
(joint work with Michel Galley, Kevin Knight, and Daniel Marcu)<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    30 Jan 04
  </td><td align=left valign=top>
    Paul Kingsbury (Penn)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jan_30');">
    PropBank: the next stage of Treebank <b>and</b><br>Inducing a Chronology of the Pali Canon
    </a><br>
  <span id=abs04_Jan_30 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
PropBank: the next stage of Treebank<p>
Natural-language engineers the world over are coming to a consensus that a
degree of semantic knowledge is a necessary addition to purely structural
representations of language.  This talk describes the Propbank project at
Penn, which provides a complete shallow semantic parse of the Treebank II
corpus.<p>
Inducing a Chronology of the Pali Canon:<p>
Works such as Kroch (1989), Taylor (1994) and Han (2000) have demonstrated
that syntactic change can be described mathematically as the competition
between innovating and archaic formations.  This paper demonstrates how
this same mathematical description can be turned around to predict the
date of a historical text.  The Middle Indic period showed dramatic change
in the morphological system, such as the collapse of the past-tense verbal
system.  Whereas Sanskrit had three competing formations, each with
multiple possible morphological realizations, Pali (a Middle Indo-Aryan
language) had only a single formation, based mostly on the sigmatic aorist
although many archaic nonsigmatic aorists are also attested.  The
proportions of the archaic and innovative forms can be easily calculated
for each text in the Pali Canon and these proportions used to assign an
approximate date for each text.  The accuracy of the method can be
assessed qualitatively by comparing the derived chronology to chronologies
based on various non-linguistic criteria, or quantitatively by comparing
the derived chronology to a known dating scheme.  For the latter it is
necessary to turn to a different dataset, such as that describing the rise
of do-support in Early Modern English, as described in Ellegard (1953) and
Kroch (1989).<p>
Bio:<p>
Paul Kingsbury graduated summa cum laude in linguistics from Ohio State
University in 1993 with a thesis on "Some sources for L-words in
Sanskrit".  He subsequently entered the University of Pennsylvania to
study historical linguistics and Sanskrit, but (like most historical
students) was diverted to computational issues.  He joined the Propbank
project in 2000 and soon thereafter engineered a major rethinking of the
methods and goals of the project, in order to make the annotation
linguistically meaningful.  He completed his doctorate in 2002 with a
thesis entitled 'The Chronology of the Pali Canon: the case of the
aorist'.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 Jan 04
  </td><td align=left valign=top>
    John Prager (IBM)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs04_Jan_16');">
    Using Constraints to Improve Question-Answering Accuracy
    </a><br>
  <span id=abs04_Jan_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:00 pm - 3:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Leading Question-Answering systems employ a variety of means to boost the
accuracy of their answers.  Such methods include redundancy (getting the
same answer from multiple documents/sources), deeper parsing of questions
and texts (hence improving the accuracy of confidence measures),
inferencing (proving the answer from information in texts plus background
knowledge) and sanity-checking (verifying that answers are consistent with
known facts).  To our knowledge, however, no QA system deliberately asks
additional questions in order to derive constraints on the answers to the
original questions.<p>
We present in this talk the method of QA-by-Dossier-with-Constraints (QDC).
This is an extension of the simpler method of QA-by-Dossier, in which
definitional questions ("Who/what is X") are addressed by asking a set of
questions about anticipated properties of X.  In QDC, the collection of
Dossier candidate answers, along with possibly other answers to questions
asked expressly for this purpose, are subjected to satisfying a set of
naturally-arising constraints.  For example, for a "Who is X" question, the
system will ask about birth, accomplishment and death dates, which, if they
exist, must occur in that order, and also obey other constraints such as
lifespan.  Temporal, spatial and kinship relationships seem to be
particularly amenable to this treatment, but it would seem that almost any
"factoid" question can benefit from QDC.  We will discuss the setting-up
and application of constraint networks, and talk about how (and whether) to
develop the constraint sets automatically.  We will demonstrate several
applications of QDC, and present one evaluation in which the F-measure for
a set of questions improved with QDC from .39 to .69.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    19 Dec 03
  </td><td align=left valign=top>
    Robert Krovetz (Ask Jeeves)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Dec_19');">
    More than One Sense Per Discourse
    </a><br>
  <span id=abs03_Dec_19 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
    Previous research has indicated that when a polysemous word appears two
    or more times in a discourse, it is extremely likely that they will all
    share the same sense (Gale et al. 92). However, those results were
    based on a coarse-grained distinction between senses (e.g, {\em
    sentence} in the sense of a `prison sentence' vs. a `grammatical
    sentence'). I conducted an analysis of multiple senses within two
    sense-tagged corpora, Semcor and DSO. These corpora used WordNet for
    their sense inventory. I found significantly more occurrences of
    multiple-senses per discourse than reported in (Gale et al. 92) (33\%
    instead of 4\%). I also found classes of ambiguous words in which as
    many as 45\% of the senses in the class co-occur within a document. I
    will discuss the implications of these results for the task of 
    word-sense tagging and for the way in which senses should be  
    represented.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Nov 03
  </td><td align=left valign=top>
    Hang Li (MSR Beijing)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Nov_25');">
    Using Bilingual Data to Mine and Rank Translations
    </a><br>
  <span id=abs03_Nov_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 pm - 12:00 pm<br>
<b>Location:</b> 11th Floor Large<br>
<b>Abstract:</b> <br>
In this talk, I will introduce some of the technologies which
we have developed in the project on an English reading assistant system
called English Reading Wizard. The technologies include a method for
mining translations from web (unparallel corpora), a method for word
translation disambiguation based on bootstrapping, which is called
Bilingual Bootstrapping, and a general method of bootstrapping, which is
called Collaborative Bootstrapping. First, I will introduce the main
features of English Reading Wizard. Next, I will introduce each of the
methods. The translation mining method is based on a nave Bayesian
ensemble and the EM algorithm. Bilingual Bootstrapping uses the
asymmetric translation relationship between words in the two languages
in translation and can construct reliable classifiers for word
translation disambiguation. Collaborative Bootstrapping contains the
co-training algorithm as its special case, and it uses the strategy of
uncertainty reduction in training of the two classifiers.<p>
Bio:<p>
Hang Li is a researcher at the Natural Language Computing Group
of Microsoft Research in Beijing, China. He is also adjunct professor of
Xian Jiaotong University. Hang Li obtained a B.S. in Electrical
Engineering from Kyoto University (Japan) in 1988 and a M.S. in Computer
Science from Kyoto University in 1990. He earned his Ph.D. in Computer
Science from the University of Tokyo in 1998. >From 1990 to 2001, Hang
Li worked at the Research Laboratories of NEC Corporation in Kawasaki,
Japan. He joined Microsoft Research in 2001. His research interest
includes statistical learning, natural language processing, data mining,
and information retrieval. Hang Li's web site:
  http://research.microsoft.com/users/hangli/<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Nov 03
  </td><td align=left valign=top>
    Dr. Kato and Dr. Fukomoto (NTCIR)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Nov_17');">
    An Overview of the QA Challenge + NTCIR -- The Way Ahead
    </a><br>
  <span id=abs03_Nov_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:30 am - 12:00 pm<br>
<b>Location:</b> 4th Floor<br>
<b>Abstract:</b> <br>
An Overview of Question Answering Challenge
Jun'ichi Fukumoto and Tsuneaki Kato<p>
In this talk, we will present an overview of Question Answering
Challenge(QAC), which is the question answering task of the NTCIR
Workshop.  QAC-1 (the first evaluation of QAC) was carried out
at NTCIR Workshop 3 in October 2002, and QAC-2 will be at
NTCIR Workshop 4 in December 2003.  In the QAC, systems to be
evaluated are expected to return exact answers consisting of a noun
or noun compound denoting, for example, the names of persons,
organizations, or various artifacts or numerical expressions such
as money, size, or date.  Those basically range over the Named
Entity (NE) elements of MUC and IREX but is not limited to them.
  QAC consists of three kinds of subtasks: Task 1, where the systems
are allowed to return ranked five possible answers; Task 2, where
the systems are required to return a complete list of answers; and
Task 3, the systems are required to answer series of questions, that
have anaphora and zero-anaphora.  We will present the results of
QAC-1, and vision and prospect of QAC-2.<p>
NTCIR -- the Way Ahead
Noriko Kando<p>
Dr. Noriko Kando is the leader of NTCIR(Test Collections and Evaluation
of IR, Text Summarization, Q&A, etc) project, and an associate professor
of National Institute of Informatics (NII).  She got her Ph. D in 1995
from Keio University.  Her research interest includes evaluation of
information retrieval systems, technologies to "Make Information Usable
for Users", cross-lingual information retrieval, and analysis of text
structure, genre, citation & link  She is a member of editorial boards of
International Journal on Information Processing and Management,
ACM-Transaction on Asian Language Information Processing, etc.<p>
Jun'ichi Fukumoto and Tsuneaki Kato are task organizers of QAC.
  Dr. Jun'ichi Fukumoto is an associate professor of Ritsumeikan
University.  He got his Ph. D in 1999 from University of Manchester
Institute of Science and Technology.  His research interest includes
Q&A, automatic summarization, and dialogue processing.
Dr. Tsuneaki Kato is an associate professor of the University of Tokyo.
He got his Dr. of Engineering in 1995 from Tokyo Institute of
Technology.  His research interests includes multimodal dialogue
processing, multimodal presentation generation and domain independent
question and answering.  He is a member of editorial committee of
transaction on information and systems of The Institute of Electronics,
Information and Communication Engineers.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 Oct 03
  </td><td align=left valign=top>
    Christopher Manning (Stanford)
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Oct_27');">
    Natural Language Parsing: Graphs, the A* Algorithm, and Modularity
    </a><br>
  <span id=abs03_Oct_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 10:00 am - 11:00 am<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Probabilistic parsing methods have in recent years transformed our ability to
robustly find correct parses for open domain sentences.  Much of this work has
been within a common architecture of heuristic search for good pares in
lexicalized probabilistic context-free grammars, with many layers of back-off
to avoid problems of sparse data. <p>
In this talk, I will outline some different ideas that we have been pursuing. 
I will connect stochastic parsing with finding shortest paths in hypergraphs,
and show how this approach naturally provides a chart parser for arbitrary
probabilistic context-free grammars (finding shortest paths in a hypergraph is
easy; the central problem of parsing is that the hypergraph has to be
constructed on the fly). From this viewpoint, a natural approach is to use the
A* algorithm to cut down the work in finding the best parse. On unlexicalized
grammars, this can reduce the parsing work done dramatically, by at least 97%.
This approach is competitive with methods standardly used in statistical
parsers, while ensuring optimality, unlike most heuristic approaches to
best-first parsing. <p>
Finally, I will present a novel modular generative model in which semantic
(lexical dependency) and syntactic structures are scored separately. This
factored model is conceptually simple, linguistically interesting, admits exact
inferenence with an extremely effective A* algorithm, and provides
straightforward opportunities for separately improving the component models. In
particular, I will mention some of the work we have done focusing on the PCFG
component to produce a very high accuracy unlexicalized grammar. <p>
This is joint work with Dan Klein. <p>
About the Speaker:<p>
Christopher Manning is an Assistant Professor of Computer Science and
Linguistics at Stanford University. He received his Ph.D. from Stanford
University in 1995, and served on the faculty of the Computational Linguistics
Program at Carnegie Mellon University (1994-1996) and the University of Sydney
Linguistics Department (1996-1999) before returning to Stanford. His research
interests include probabilistic models of language, natural language parsing,
constraint-based linguistic theories, syntactic typology, information
extraction and text mining, and computational lexicography. He is the author of
three books, including Foundations of Statistical Natural Language Processing
(MIT Press, 1999, with Hinrich Schuetze). <p>
Chris' schedule is available in <a href="manning.ps">Postscript</a> or
<a href="manning.pdf">PDF</a> format.

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    17 Oct 03
  </td><td align=left valign=top>
    Hovy, Marcu, Knight, Byrd, Narayanan, Traum, Gordon
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Oct_17');">
    Introduction to CL Research
    </a><br>
  <span id=abs03_Oct_17 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:30 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The annual Computational Linguistics Open House will be held at USC's Information
Sciences Institute from 3:00-4:30pm in the 11th floor Conference Room. Researchers from
ISI, including Eduard Hovy, Daniel Marcu, and Kevin Knight will present overviews of
their latest research.  We will also hear about the research activities of Dani Byrd of
the Linguistics Department, Shri Narayanan's group in EE, and David Traum and Andrew
Gordon of USC's Institute for Creative Technologies.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    10 Oct 03
  </td><td align=left valign=top>
    Philipp Koehn
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Oct_10');">
    Advances in Statistical MT: Phrases, Noun Phrases and Beyond
    </a><br>
  <span id=abs03_Oct_10 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
(This is a practice run for I talk I will give a few times over the next
weeks when interviewing for job positions.)<p>
I will review the state of the art in statistical machine translation
(SMT), present my dissertation work, and sketch out the research
challenges of syntactically structured statistical machine translation.<p>
The currently best methods in SMT build on the translation of phrases (any
sequences of words) instead of single words. Phrase translation pairs are
automatically learned from parallel corpora. While SMT systems generate
translation output that often conveys a lot of the meaning of the original
text, it is frequently ungrammatical and incoherent.<p>
The research challenge at this point is to introduce syntactic knowledge
to the state of the art in order to improve translation quality. My
approach breaks up the translation process along linguistic lines. I will
present my thesis work on noun phrase translation and ideas about clause
structure.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    03 Oct 03
  </td><td align=left valign=top>
    Anton Leuski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Oct_03');">
    A Year in Paradise
    </a><br>
  <span id=abs03_Oct_03 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I would like to talk about some of the things I did during the last 
year. I will discuss and demonstrate CuSTaRD, a cross-lingual 
information retrieval, organization, summarization, and visualization 
system that was built for the Surprise Language exercise. I will focus 
in more details on iNeATS, the interactive multi-document summarization 
part of CuSTaRD. The other project I plan to present is eArchivarius, a 
system for accessing collections of electronic mail.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 Oct 03
  </td><td align=left valign=top>
    Ana-Maria Popescu
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Oct_02');">
    TBA
    </a><br>
  <span id=abs03_Oct_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 4:00 pm - 5:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Sep 03
  </td><td align=left valign=top>
    Beata Klebanov
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Sep_15');">
    Analyzing Sentences into Facts: Simple is Beautiful
    </a><br>
  <span id=abs03_Sep_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I present my summer project  - writing rule-based software for
simplifying texts. Task definition and motivations will be
discussed, as well as human and automatic evaluation, the
latter using a question answering system.<p>
This is joint work with Daniel Marcu and Kevin Knight.<p>

<p><b>Slides:</b> 
<a href="slides/klebanov_facts.ppt">klebanov_facts.ppt</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Sep 03
  </td><td align=left valign=top>
    Lara Taylor
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Sep_12');">
    Discourse Coherence for Ordering Information
    </a><br>
  <span id=abs03_Sep_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 2:30 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this talk, I look at how the notion of discourse coherence can be
modeled computationally. I begin with the following idea: if you take
a text and shuffle its sentences into a random order, that text will
no longer make sense. In other words, the text will be "incoherent".
Our task is to learn how to reassemble a shuffled text into an order
that humans would consider to be coherent.<p>
I discuss practical and theoretical motivations for the task,
evaluations of our model, increases in performance achieved over the
summer, and directions for future research.<p>
This work was done in collaboration with Kevin Knight, Daniel Marcu,
Jonathan Graehl and Nick Mote.<p>

<p><b>Slides:</b> 
<a href="slides/taylor_ordering.ppt">taylor_ordering.ppt</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    05 Sep 03
  </td><td align=left valign=top>
    Nishit Rathod and Anish Nair
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Sep_05');">
    Deciphering Hindi Scripts
    </a><br>
  <span id=abs03_Sep_05 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
A major hurdle in building automated information retrieval systems for
Hindi text is the lack of an uniform encoding for text representation.
Standards do exist, but noone seems interested. Every web content
publisher seems to have their encoding system, making information
extraction a nightmare. We explore an unsupervised approach to
convert any given "unknown" encoding to UTF-8, by treating it as a
decipherment problem. We also study how a little amount of supervision
can improve decoding accuracy.<p>

<p><b>Slides:</b> 
<a href="slides/rathod+nair_hindi.pdf.bz2">rathod+nair_hindi.pdf.bz2</a> 
<a href="slides/rathod+nair_hindi.ppt">rathod+nair_hindi.ppt</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    03 Sep 03
  </td><td align=left valign=top>
    Alex Fraser and Franz Och
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Sep_03');">
    JHU MT Workshop
    </a><br>
  <span id=abs03_Sep_03 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We will present the results of the 2003 Johns Hopkins University
Summer Workshop on "Syntax for Statistical Machine Translation".<p>
We will describe a large effort to extend a high-performing
phrase-based MT system as baseline by adding new features representing
syntactic knowledge that deal with specific problems of the underlying
baseline. We investigate a broad range of possible feature functions,
from very simple binary features to sophisticated tree-to-tree
translation models. Simple feature functions test if a certain
constituent occurs in the source and the target language parse
tree. More sophisticated features will be derived from an alignment
model where whole sub-trees in source and target can be aligned node
by node. We present results on the Chinese-English large data track of
the recent TIDES MT evaluations.<p>
This is joint work with the other workshop team members: Daniel
Gildea, Anoop Sarkar, Sanjeev Khudanpur, Kenji Yamada, Libin Shen,
Shankar Kumar, David Smith, Viran Jain, Katherine Eng, Jin Zhen and
Dragomir Radev.<p>
See <a
href="http://www.clsp.jhu.edu/ws03/groups/translate/">http://www.clsp.jhu.edu/ws03/groups/translate/</a>
for more.<p>

<p><b>Slides:</b> 
<a href="slides/fraser_mt.pdf.bz2">fraser_mt.pdf.bz2</a> 
</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    29 Aug 03
  </td><td align=left valign=top>
    Stefan Riezler
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Aug_29');">
    Deepening Representations
    </a><br>
  <span id=abs03_Aug_29 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 Aug 03
  </td><td align=left valign=top>
    Michel Galley and Mark Hopkins
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Aug_27');">
    Syntax for Statistical MT
    </a><br>
  <span id=abs03_Aug_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    22 Aug 03
  </td><td align=left valign=top>
    Satoshi Sekine
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Aug_22');">
    Information Extraction, IR and QA
    </a><br>
  <span id=abs03_Aug_22 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    15 Aug 03
  </td><td align=left valign=top>
    Beata Klebanov
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Aug_15');">
    On Her Masters Research
    </a><br>
  <span id=abs03_Aug_15 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    01 Aug 03
  </td><td align=left valign=top>
    Shou-de Lin
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Aug_01');">
    Toward deciphering the 2-dimensional ancient Luwian script by discovering its writing order
    </a><br>
  <span id=abs03_Aug_01 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    29 Jul 03
  </td><td align=left valign=top>
    Michael Brasser
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jul_29');">
    A Model of Word Movement for Machine Translation
    </a><br>
  <span id=abs03_Jul_29 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Small<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Jul 03
  </td><td align=left valign=top>
    Jonathan Graehl and Kevin Knight
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jul_25');">
    Super-Carmel for Trees
    </a><br>
  <span id=abs03_Jul_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    18 Jul 03
  </td><td align=left valign=top>
    Doug Oard
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jul_18');">
    A Maryland Yankee in King Eduard's Court: Some Remarks on a Year in Paradise
    </a><br>
  <span id=abs03_Jul_18 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    27 Jun 03
  </td><td align=left valign=top>
    Michael Fleischman
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jun_27');">
    Offline Strategies for Online Question Answering: Answering Questions Before They Are Asked and Maximum Entropy Models for FrameNet Classification
    </a><br>
  <span id=abs03_Jun_27 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 10 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    12 Jun 03
  </td><td align=left valign=top>
    Dina Demner-Fushman
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jun_12');">
    Measuring the Effect of Dictionary Coverage on Cross-Language Retrieval
    </a><br>
  <span id=abs03_Jun_12 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 11:00 am - 12:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Bilingual term lists have proven to be a useful basis for
dictionary-based Cross-Language Information Retrieval (CLIR), but
there is ample anecdotal evidence that differences in vocabulary
coverage can have a substantial impact on retrieval effectiveness.
This issue has recently been explored using ablation studies in which
progressively smaller term lists were synthesized using sampling
techniques. The ablation techniques used in those studies have not,
however, been validated using real terms lists. In this talk I will
report the results of what we believe is the first large coverage
study use naturally occurring term lists. Thirty-five bilingual terms
lists were obtained from a variety of sources, each with English as
one of the two paired languages. From these, we created 35
English-to-English term lists by taking each term that was present in
the English side of the list as its own translation. When used with
an English information retreval test collection, this allowed us to
measure the reduction in retrieval effectivenss that could be
attributed to deficiencies in the coverage of English terms. Eight
types of untranslatable terms were identified in a collection of news
stories, of which named entitles were found to have the greatest
impact on retrieval effectiveness. Differences in named entity
coverage were found to produce large differences in retrieval
effectiveness for term lists of similar sizes. Controlling for named
entity effects yielded a clear relationship between retrieval
effectiveness and the size of the translatable English vocabulary.
The functional dependence that we observed is consistent with one
previously applied ablation technique and inconsistent with another.
Our results indicate that the outcome of a widely cited landmark study
of query expansion effects for CLIR was likely affected by a flawed
ablation model. We conclude our talk with a suggestion for further
work on that topic, and a simple prescription for avoiding such
problems in the future.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    23 May 03
  </td><td align=left valign=top>
    Liang Zhou
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_May_23');">
    A Web-Trained Extraction Summarization System and Headline Summarization at ISI
    </a><br>
  <span id=abs03_May_23 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
1) A serious bottleneck in the development of trainable text summarization
systems is the shortage of training data. Constructing such data is a very
tedious task, especially because there are in general many different
correct ways to summarize a text. Fortunately we can utilize the Internet
as a source of suitable training data. In this paper, we present a
summarization system that uses the web as the source of training data. The
procedure involves structuring the articles downloaded from various
websites, building adequate corpora of (summary, text) and (extract,
text) pairs, training on positive and negative data, and automatically
learning to perform the task of extraction-based summarization systems.<p>
2) Headlines are useful for users who only need information on the main
topics of a story. We present a headline summarization system that is
built at ISI for this purpose and is a top performer for DUC2003's task 1,
generating very short summaries (10 words or less). <p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    20 May 03
  </td><td align=left valign=top>
    Michel Galley
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_May_20');">
    Discourse Segmentation of Multi-Party Conversation
    </a><br>
  <span id=abs03_May_20 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    16 May 03
  </td><td align=left valign=top>
    Chin-Yew Lin
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_May_16');">
    Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics
    </a><br>
  <span id=abs03_May_16 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Following the recent adoption by the machine translation community of
automatic evaluation using the BLEU/NIST scoring process, we conduct an
in-depth study of a similar idea for evaluating summaries. The results
show that automatic evaluation using unigram co-occurrences between
summary pairs correlates surprising well with human evaluations, based
on various statistical metrics; while direct application of the BLEU
evaluation procedure does not always give good results.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    09 May 03
  </td><td align=left valign=top>
    Doug Oard
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_May_09');">
    Coping with Surprise: The Case of Cebuano
    </a><br>
  <span id=abs03_May_09 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
For ten days in March, nine research teams worked together to build
Cebuano language resources and systems for a "dry run" the TIDES Suprise
Language experiment. Cebuano is spoken widely in the southern
Phillipines, but there had previously been little work on computational
linguistics for that language. As we prepare for the actual Suprise
Language experiment this June, we will use this talk to look back on what
worked, what didn't, and what lessons there are to be learned from our
experience in March. Come prepared to share the excitement, offer your
ideas, and understand why we have tried to ask Ed to cancel all vacations
during the month of June (just kidding...).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    02 May 03
  </td><td align=left valign=top>
    Hal Daum&eacute; III
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_May_02');">
    Acquiring Paraphrase Templates from Document/Abstract Pairs
    </a><br>
  <span id=abs03_May_02 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We present an approach to automatically extracting paraphrase templates
from document/abstract pairs. This methodology relies on word-based
alignments created by off-the-shelf software. Our paraphrases are
evaluated by human evaluators for precision and automatically for
applicability. We find that 77% of the extracted paraphrases are judged
to be always correct and that the generalized templates of 60% are
judged to be applicable most of the time and 87% are judged to be
applicable sometimes.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    25 Apr 03
  </td><td align=left valign=top>
    Quamrul Tipu
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Apr_25');">
    Statistical MT with Bilingual Morphology
    </a><br>
  <span id=abs03_Apr_25 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Traditional statistical MT systems mostly work on the word-
andphrase-level. For different language pairs, the performance of such
systems vary from some 15% to 35%. These systems suffer from problems
such as sparse data, with huge vocabulary sizes leading to less
reliable probability estimates. In our current research, we aim to
come up with a better MT system by looking inside the words. Almost in
every language, a root (stem) can have many different forms
(inflectional, derivational, etc.). If we can identify the roots, the
size of the vocabulary will quite small, and we can have better
probability estimates, reducing the sparse data problem and
potentially leading to higher accuracy. We are trying to come up with
a model that induces morphology automatically from a bilingual corpus
and achieves this improvement.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    04 Apr 03
  </td><td align=left valign=top>
    Donghui Feng
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Apr_04');">
    Natural Language Understanding in MRE
    </a><br>
  <span id=abs03_Apr_04 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
In this talk, I will present my current work on language understanding
in the project, Mission Rehearsal Exercise(MRE). One of the challenges
in a dialogure system is to provide a robust understanding/parsing
compoment. We applied both Finte State Model and Statistical Learning
Model for the parsing of separate sentences of dialogue utterances.
Their performances are evaluated and compared with a new blind set.
And we hope to incorporate them to make a better solution in this
specific application.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    21 Mar 03
  </td><td align=left valign=top>
    Gareth Jones
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Mar_21');">
    An Investigation of the Application of Broad Coverage Automatic Pronoun Resolution in Information Retrieval
    </a><br>
  <span id=abs03_Mar_21 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Term weighting methods have been shown to give significant increases
in information retrieval performance. Term weights are typically
calculated using frequency counts across the whole retrieval
collection, frequency of each term within individual documents and
compensation for varying document length. The presence of pronomial
references in documents effectively reduces the within document term
frequency of associated words with a consequent effect on term weights
and information retrieval behaviour. This presentation will describe
an experimental investigation into the impact on information retrieval
performance of broad coverage automatic pronoun resolution. Results
using a standard information retieval test collection indicate that
calculating term weights using a pronoun resolved version of the
document test collection can improve both fixed cutoff and average
retrieval precision.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    14 Mar 03
  </td><td align=left valign=top>
    Kareem Darwish
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Mar_14');">
    Improving the Efficiency and Effectiveness of Structured Query Methods
    </a><br>
  <span id=abs03_Mar_14 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
One of the key challenges in retrieval is what to do when a query term
needs to be replaced with more than one term. This problem arises in
applications such as cross language information retrieval and
thesaurus expansion. One solution is to use structured query methods,
which treat all the possible replacements as if they were one query
term by computing a joint document frequency and a joint term
frequency. This presentation will review prior work on structured
query techniques and then introduce three new variants that aim to
improve computational efficiency and to leverage estimates of
replacement probabilities to improve retrieval effectiveness. The
methods have now been tested in cross-language retrieval and
OCR-degraded text retrieval applications in which replacement
probability estimates could be estimated. In both applications, the
new structured query methods showed statistically significant
improvements in retrieval effectiveness over previously known
structured query methods.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    07 Mar 03
  </td><td align=left valign=top>
    Scott Klemmer
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Mar_07');">
    Books with Voices: Paper Transcripts as a Tangible Interface to Oral Histories
    </a><br>
  <span id=abs03_Mar_07 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Our contextual inquiry into the practices of oral
historians unearthed
a curious incongruity. While oral historians consider interview
recordings a central historical artifact, these recordings
sit unused
after a written transcript is produced. We hypothesized
that this is
largely because books are more usable than recordings.
Therefore, we
created Books with Voices: bar-code augmented paper transcripts
enabling fast, random access to digital video interviews on
a PDA. We
present quantitative results of an evaluation of this tangible
interface with 13 participants. They found this lightweight,
structured access to original recordings to offer
substantial benefits
with minimal overhead. Oral historians found a level of
emotion in the
video not available in the printed transcript. The video
also helped
readers clarify the text and observe nonverbal cues.<p>
<a
href="http://guir.berkeley.edu/oral-history/">http://guir.berkeley.edu/oral-history/<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    28 Feb 03
  </td><td align=left valign=top>
    Radu Soricut
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Feb_28');">
    Sentence Level Discourse Parsing using Syntactic and Lexical Information
    </a><br>
  <span id=abs03_Feb_28 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
We introduce two probabilistic models that can be used to identify 
elementary discourse units and build sentence-level discourse parse 
trees. The models use syntactic and lexical features. A discourse parsing
algorithm that implements these models derives discourse parse trees with
an error reduction of 18.8\% over a state-of-the-art decision-based
discourse parser. A set of empirical evaluations shows that our discourse
parsing model is sophisticated enough to yield discourse trees at an
accuracy level that matches near-human levels of performance.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    21 Feb 03
  </td><td align=left valign=top>
    Nate Chambers
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Feb_21');">
    Statistical Language Generation in a Dialogue System
    </a><br>
  <span id=abs03_Feb_21 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
The large corpora of written text that is available to the language
community has largely been utilized for language understanding; it has
somewhat been ignored in the context of language generation. Recent
developments in stochastic generation have allowed such systems to shift
the burden from hand crafted databases (lexicons, grammars, ontologies) to
the knowledge implicitly found in written text. However, when building a
dialogue system, generation is largely interactive, very different from
the written structure of most corpora.<p>
In this talk, I will discuss my recent work at applying a stochastic
generator, HALogen, and its newswire language model to a dialogue system,
TRIPS. I'll describe the difficulties in mapping the TRIPS semantic form
into HALogen's representation, the critical differences between newswire
and dialogue, and the possibility of using HALogen and a large newswire
model as a domain independent generator. <p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    07 Feb 03
  </td><td align=left valign=top>
    Jeongwon Cha
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Feb_07');">
    Automatic Pattern Learning for Information Extraction using Web Data
    </a><br>
  <span id=abs03_Feb_07 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will give a status report work on information extraction during last
10 months. The motivation of this work is to learn extraction
patterns automatically using seed template and web search engine. My
approach is to generate linguistics patterns and surface patterns and
combine them to compenstate for the respective weaknesses of two
patterns. On the DUC01-test-disasters (67 documents),
DUC01-training-disasters (54 documents) I got a 0.34/0.26 f-measure
respectively. In this talk, I will give a status report on ReAD
project (with Dr. Chin-Yew Lin).<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    31 Jan 03
  </td><td align=left valign=top>
    Philipp Koehn
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jan_31');">
    Noun Phrase Translation
    </a><br>
  <span id=abs03_Jan_31 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
I will give a status report on my current thesis work on
noun phrase translation. The motivation of this work is
to break up the machine translation problem into smaller,
more manageable units. The treatment of noun phrase translation
as a subtask of machine translation is both linguistically
and empirically motivated. My approach is to generate 
a n-best list of candidate translations with a statistical 
machine translation system and rerank the candidates with
additional features. For about 90% of all noun phrases we
can find an acceptable translation in the 100-best list, while 
an acceptable translation comes out on the very top for only 
about 60% of the noun phrases. I will discuss a variety of 
linguistic and empirical features that (may) help to move 
the acceptable translations higher in the list. I will also
present results modeling issues such as phrase based 
translation and compound splitting. This talk is also 
intended as a fishing expedition for feature suggestions by
the audience.<p>

</font>
</span>
</td></tr>
  <tr><td align=left valign=top>
    24 Jan 03
  </td><td align=left valign=top>
    Doug Oard &amp; Anton Leuski
  </td><td align=left valign=top>
    <a onMouseOver="window.status='View abstract'; return true" onMouseOut="window.status=' '; return true" href="javascript:exp_coll('abs03_Jan_24');">
    Access to Archival Collections of Electronic Mail
    </a><br>
  <span id=abs03_Jan_24 style="display:none;">
  <font face=Arial size=-1>
<b>Time:</b> 3:00 pm - 4:00 pm<br>
<b>Location:</b> 11 Large<br>
<b>Abstract:</b> <br>
Since its inception more than 30 years ago, electronic mail (email)
has developed into a powerful communication medium with applications
that extend well beyond simple asynchronous message exchange between
individuals. Automated tools to support the use of email in
individual, organizational and social contexts have received
increasing attention in recent years. Among the tasks that are now
supported are filtering (e.g., spam detection), aggregation (e.g.,
mailing list digests), workflow management (e.g., help desk routing),
and reuse (e.g., retrospective search). We are interested in how
today's email will be used in the future -- some will certainly be
preserved (indeed, some MUST be preserved!), and those records will
serve as powerful evidence of how we lived our lives and organized our
societies. The challenges of managing many types of electronic record
collections are receiving increasing attention, but we are not aware
of any work yet on supporting access to electronic mail archives.
That will be the focus of this talk.<p>
We will introduce the Open Archival Information Systems (OAIS) model,
and then focus on two key processes: ingestion and access. Our focus
in ingestion is on support for review and redaction, which we believe
will be key enablers to acquisition and near-term access. For access,
we will address both browsing based on provenance (original order) and
user-guided reorganization based on search and visualization. Along
the way, we will identify potentially productive opportunities to
apply natural language processing technologies such as topic
segmentation, link detection, and summarization. We will then
describe two test collections, and demonstrate a system that we have
developed to explore user-guided reorganization through visualization
for one of those collections. We will conclude the talk by sketching
out a research agenda. At that point, we will expect suggestions and
comments from the audience. Knowing this audience, it is unlikely
that we will need to wait that long :-).<p>

</font>
</span>
</td></tr>
</table>



<div align="center"><font face="Verdana, Arial, Helvetica, sans-serif" size="1">
This web page was last generated at Fri Aug  8 14:06:42 PDT 2008<br>
Comments, corrections? email <a href="mailto:sdenee[nospam}fe@isi.edu"><script>document.write("sd"+"en"+"ee" + "fe@i"+"si.e"+"du")</script></a></font></div>


</body>
</html>
